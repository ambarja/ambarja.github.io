{
  "hash": "ebb8c128370ff9de3172ff8e08f4fa95",
  "result": {
    "markdown": "---\ntitle: \"Naive Bayes with Tidymodels\"\ndescription: |\n  Using Tidymodels to classify Netflix dataset with the help of Naive Bayes classifier.\nauthor:\n  - name: Karat Sidhu\n    url: {}\ndate: 2022-05-28\nimage: images/logo.jpg\ncategories:\n  - Naive Bayes\n  - TidyModels\n  - Machine Learning\n  - Classification\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\ncode-fold: true\n\n---\n\n\n\n# Naive Bayes on Netflix Dataset\n\nThis is part 4 of my Tidymodels learning series. Part 1 dealt with SVM& Random Forests^[https://karatsidhu.com/posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html], part 2 I tried out \nPCA analysis and UMAP^[https://karatsidhu.com/posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html] and part 3 I wanted to try KNN and GLM modelling^[https://karatsidhu.com/posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html].\n\nFor the next part, I wanted to try something I usually don't have to run in metabolomics data, so I went with Naive Bayes.\n\nData set was used from kaggle.com^[https://www.kaggle.com/datasets/shivamb/netflix-shows].\n\nNaive Bayes is a basic algorithm for classifying data. It is not often used in my field but is a useful skill to have.\n\n# Loading the packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 0.2.0 ──\n```\n:::\n\n::: {.cell-output-stderr}\n```\n✔ broom        0.8.0     ✔ rsample      0.1.1\n✔ dials        0.1.1     ✔ tune         0.2.0\n✔ infer        1.0.0     ✔ workflows    0.2.6\n✔ modeldata    0.1.1     ✔ workflowsets 0.2.1\n✔ parsnip      0.2.1     ✔ yardstick    0.0.9\n✔ recipes      0.2.0     \n```\n:::\n\n::: {.cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidytext) # text splitting and analysis\nlibrary(naivebayes)\n```\n\n::: {.cell-output-stderr}\n```\nnaivebayes 0.9.7 loaded\n```\n:::\n\n```{.r .cell-code}\nlibrary(textrecipes)\nlibrary(themis) # for unbalance data\n```\n\n::: {.cell-output-stderr}\n```\n\nAttaching package: 'themis'\n```\n:::\n\n::: {.cell-output-stderr}\n```\nThe following objects are masked from 'package:recipes':\n\n    step_downsample, step_upsample\n```\n:::\n\n```{.r .cell-code}\nlibrary(discrim) # for discriminant analysis\n```\n\n::: {.cell-output-stderr}\n```\n\nAttaching package: 'discrim'\n```\n:::\n\n::: {.cell-output-stderr}\n```\nThe following object is masked from 'package:dials':\n\n    smoothness\n```\n:::\n:::\n\n# Loading the data\n\n::: {.cell}\n\n```{.r .cell-code}\nnetflix <- readr::read_csv(\"netflix_titles.csv\")\n```\n\n::: {.cell-output-stderr}\n```\nRows: 8807 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): show_id, type, title, director, cast, country, date_added, rating,...\ndbl  (1): release_year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\n# Using a subset of data\n\nI am running it on my local machine and it takes a while to do 8000+ rows, so I decided to use only a small set of data\n\n::: {.cell}\n\n```{.r .cell-code}\nnetflix<- netflix  |> slice_head(n = 600)\n```\n:::\n\n\n\n# Exploratory data analysis\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts <-\n  netflix |>\n  distinct(show_id, .keep_all = TRUE) |>\n  unnest_tokens(words, description) |>\n  count(type, words, sort = TRUE)\n\n\ncounts\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 5,261 × 3\n   type    words     n\n   <chr>   <chr> <int>\n 1 Movie   a       597\n 2 Movie   the     368\n 3 Movie   to      318\n 4 TV Show a       274\n 5 Movie   and     241\n 6 Movie   of      221\n 7 Movie   in      193\n 8 TV Show the     191\n 9 TV Show and     185\n10 TV Show to      152\n# … with 5,251 more rows\n```\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncounts |>\n  tidylo::bind_log_odds(type, words, n) |>\n  filter(n > 10) |>\n  group_by(type) |>\n  slice_max(log_odds_weighted, n = 10) |>\n  ungroup() |>\n  ggplot(aes(log_odds_weighted,\n    fct_reorder(words, log_odds_weighted),\n    fill = type\n  )) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(vars(type), scales = \"free_y\") +\n  labs(y = NULL)\n```\n\n::: {.cell-output-display}\n![](Naive_bayes_files/figure-html/unnamed-chunk-5-1.png){width=768}\n:::\n:::\n\n\n# Splitting data into train and test, bootstrapping\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\nnetflix_split <-\n  netflix |>\n  distinct(show_id, .keep_all = TRUE) |>\n  select(type, description) |>\n  initial_split(prop = 0.8, strata = type)\n\nnetflix_train <- training(netflix_split)\nnetflix_test <- testing(netflix_split)\n\nset.seed(234)\nnetflix_folds <- bootstraps(netflix_train, strata = type)\nnetflix_folds\n```\n\n::: {.cell-output-stdout}\n```\n# Bootstrap sampling using stratification \n# A tibble: 25 × 2\n   splits            id         \n   <list>            <chr>      \n 1 <split [479/173]> Bootstrap01\n 2 <split [479/185]> Bootstrap02\n 3 <split [479/174]> Bootstrap03\n 4 <split [479/165]> Bootstrap04\n 5 <split [479/172]> Bootstrap05\n 6 <split [479/175]> Bootstrap06\n 7 <split [479/177]> Bootstrap07\n 8 <split [479/173]> Bootstrap08\n 9 <split [479/180]> Bootstrap09\n10 <split [479/181]> Bootstrap10\n# … with 15 more rows\n```\n:::\n:::\n\n\n# Tokenization and prepping the recipe\n\n::: {.cell}\n\n```{.r .cell-code}\nrec_all <-\n  recipe(type ~ description, data = netflix_train) |>\n  step_tokenize(description) |>\n  step_tokenfilter(description, max_tokens = 80) |>\n  step_tfidf(description)\n\nrec_all_norm <-\n  rec_all |>\n  step_normalize(all_predictors())\n\nrec_all_smote <-\n  rec_all_norm |>\n  step_smote(type)\n\n## we can `prep()` just to check if it works\nprep(rec_all_smote)\n```\n\n::: {.cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 479 data points and no missing data.\n\nOperations:\n\nTokenization for description [trained]\nText filtering for description [trained]\nTerm frequency-inverse document frequency with description [trained]\nCentering and scaling for tfidf_description_a, tfidf_description_about, t... [trained]\nSMOTE based on type [trained]\n```\n:::\n:::\n\n\n\n## For Stop Words\n \n::: {.cell}\n\n```{.r .cell-code}\nrec_stop <-\n  recipe(type ~ description, data = netflix_train) |>\n  step_tokenize(description) |>\n  step_stopwords(description) |>\n  step_tokenfilter(description, max_tokens = 80) |>\n  step_tfidf(description)\n\nrec_stop_norm <-\n  rec_stop |>\n  step_normalize(all_predictors())\n\nrec_stop_smote <-\n  rec_stop_norm |>\n  step_smote(type)\n\n## again, let's check it\nprep(rec_stop_smote)\n```\n\n::: {.cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 479 data points and no missing data.\n\nOperations:\n\nTokenization for description [trained]\nStop word removal for description [trained]\nText filtering for description [trained]\nTerm frequency-inverse document frequency with description [trained]\nCentering and scaling for tfidf_description_back, tfidf_description_based... [trained]\nSMOTE based on type [trained]\n```\n:::\n:::\n\n\n# Naive Bayes spec\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnb_spec <-\n  naive_Bayes() |>\n  set_mode(\"classification\") |>\n  set_engine(\"naivebayes\")\n\nnb_spec\n```\n\n::: {.cell-output-stdout}\n```\nNaive Bayes Model Specification (classification)\n\nComputational engine: naivebayes \n```\n:::\n:::\n\n\n# Setting workflow for the model\n\n::: {.cell}\n\n```{.r .cell-code}\nnetflix_models <-\n  workflow_set(\n    preproc = list(\n      all = rec_all,\n      all_norm = rec_all_norm,\n      all_smote = rec_all_smote,\n      stop = rec_stop,\n      stop_norm = rec_stop_norm,\n      stop_smote = rec_stop_smote\n    ),\n    models = list(nb = nb_spec),\n    cross = TRUE\n  )\n\nnetflix_models\n```\n\n::: {.cell-output-stdout}\n```\n# A workflow set/tibble: 6 × 4\n  wflow_id      info             option    result    \n  <chr>         <list>           <list>    <list>    \n1 all_nb        <tibble [1 × 4]> <opts[0]> <list [0]>\n2 all_norm_nb   <tibble [1 × 4]> <opts[0]> <list [0]>\n3 all_smote_nb  <tibble [1 × 4]> <opts[0]> <list [0]>\n4 stop_nb       <tibble [1 × 4]> <opts[0]> <list [0]>\n5 stop_norm_nb  <tibble [1 × 4]> <opts[0]> <list [0]>\n6 stop_smote_nb <tibble [1 × 4]> <opts[0]> <list [0]>\n```\n:::\n:::\n\n\n\n# Fitting samples\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\ndoParallel::registerDoParallel()\n\nnetflix_rs <-\n  netflix_models |>\n  workflow_map(\n    \"fit_resamples\",\n    resamples = netflix_folds,\n    metrics = metric_set(accuracy, sensitivity, specificity)\n  )\n```\n:::\n\n# Ranking the results\n\n::: {.cell}\n\n```{.r .cell-code}\nrank_results(netflix_rs) |>\n  filter(.metric == \"accuracy\")\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 6 × 9\n  wflow_id      .config     .metric  mean std_err     n preprocessor model  rank\n  <chr>         <chr>       <chr>   <dbl>   <dbl> <int> <chr>        <chr> <int>\n1 all_nb        Preprocess… accura… 0.646 0.00410    25 recipe       naiv…     1\n2 stop_nb       Preprocess… accura… 0.642 0.00548    25 recipe       naiv…     2\n3 all_norm_nb   Preprocess… accura… 0.623 0.00946    25 recipe       naiv…     3\n4 stop_norm_nb  Preprocess… accura… 0.414 0.00873    25 recipe       naiv…     4\n5 all_smote_nb  Preprocess… accura… 0.390 0.00550    25 recipe       naiv…     5\n6 stop_smote_nb Preprocess… accura… 0.373 0.00389    25 recipe       naiv…     6\n```\n:::\n:::\n\n\n# Workflow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnetflix_wf <- workflow(rec_all, nb_spec)\n\nnetflix_fitted <-\n  last_fit(\n    netflix_wf,\n    netflix_split,\n    metrics = metric_set(accuracy, sensitivity, specificity)\n  )\n\nnetflix_fitted\n```\n\n::: {.cell-output-stdout}\n```\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits            id               .metrics .notes   .predictions .workflow \n  <list>            <chr>            <list>   <list>   <list>       <list>    \n1 <split [479/121]> train/test split <tibble> <tibble> <tibble>     <workflow>\n```\n:::\n:::\n\n# Final Fitted metrics\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(netflix_fitted)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  <chr>       <chr>          <dbl> <chr>               \n1 accuracy    binary        0.636  Preprocessor1_Model1\n2 sensitivity binary        0.974  Preprocessor1_Model1\n3 specificity binary        0.0233 Preprocessor1_Model1\n```\n:::\n:::\n\n\n# Confusion Matrix\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_predictions(netflix_fitted) |>\n  conf_mat(type, .pred_class) |>\n  autoplot()\n```\n\n::: {.cell-output-display}\n![](Naive_bayes_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n# Details of the full fitted model.\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_workflow(netflix_fitted)  \n```\n\n::: {.cell-output-stdout}\n```\n══ Workflow [trained] ══════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: naive_Bayes()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n3 Recipe Steps\n\n• step_tokenize()\n• step_tokenfilter()\n• step_tfidf()\n\n── Model ───────────────────────────────────────────────────────────────────────\n\n================================== Naive Bayes ================================== \n \n Call: \nnaive_bayes.default(x = maybe_data_frame(x), y = y, usekernel = TRUE)\n\n--------------------------------------------------------------------------------- \n \nLaplace smoothing: 0\n\n--------------------------------------------------------------------------------- \n \n A priori probabilities: \n\n    Movie   TV Show \n0.6450939 0.3549061 \n\n--------------------------------------------------------------------------------- \n \n Tables: \n\n--------------------------------------------------------------------------------- \n ::: tfidf_description_a::Movie (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n\tdensity.default(x = x, na.rm = TRUE)\n\nData: x (309 obs.);\tBandwidth 'bw' = 0.02165\n\n       x                  y           \n Min.   :-0.06496   Min.   :0.000017  \n 1st Qu.: 0.10494   1st Qu.:0.034413  \n Median : 0.27483   Median :0.579214  \n Mean   : 0.27483   Mean   :1.469698  \n 3rd Qu.: 0.44473   3rd Qu.:2.780068  \n Max.   : 0.61463   Max.   :4.962209  \n\n--------------------------------------------------------------------------------- \n ::: tfidf_description_a::TV Show (KDE)\n--------------------------------------------------------------------------------- \n\nCall:\n\tdensity.default(x = x, na.rm = TRUE)\n\nData: x (170 obs.);\tBandwidth 'bw' = 0.0292\n\n       x                  y           \n Min.   :-0.08760   Min.   :0.000914  \n 1st Qu.: 0.07987   1st Qu.:0.108085  \n\n...\nand 147 more lines.\n```\n:::\n:::\n\n\n\n# Further Reading\n\n- https://juliasilge.com/blog/star-trek/\n\n- https://www.tjmahr.com/bayes-theorem-in-three-panels/\n\n- https://www.kaggle.com/datasets/shivamb/netflix-shows\n\n- https://emilhvitfeldt.github.io/ISLR-tidymodels-labs/classification.html#naive-bayes\n\n\n\nYou can save this fitted `netflix_wf` object to use later with new data, for example with `readr::write_rds()`.",
    "supporting": [
      "Naive_bayes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}