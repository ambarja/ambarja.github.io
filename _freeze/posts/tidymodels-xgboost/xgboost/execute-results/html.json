{
  "hash": "01a1ab45de9b1994a31746b5576f6da6",
  "result": {
    "markdown": "---\ntitle: \"TidyModels - XGBoost\"\ndescription: |\n  Learning Tidymodels package using the Volleyball dataset from TidyTuesday.\nauthor:\n  - name: Karat Sidhu\n    url: {}\ndate: 2022-06-02\nimage: images/logo.svg\ncategories:\n  - TidyModels\n  - XGBoost\n  - Machine Learning\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\n---\n\n\n\n\n# XGBoost with TidyModels\n\nTidyTuesday Volleyball Data\n\nThis is my ongoing series^[https://karatsidhu.com/posts/netflix-naive_bayes/naive_bayes] of learning a bit more about how different algorithms in TidyModels packages work and performing some of those with the help of available material online.\n\nThe source of this code is from a Julie Silgie video of the same dataset[^1] that I used to follow along.\n\n[^1]: <https://www.youtube.com/watch?v=hpudxAmxHSM>\n\nI have never had to use XGBoost in my line of work so it was a completely new experience for me.\n\nI trimmed the data because it takes forever to run the workflow on my local machine, so the model produced is not very good.\n\nI have linked to  the original in the footnotes, please use that tutorial instead. This is very bare-bones.\n\n### The ROC curve is terrible, this post is merely for learning purposes.\n\nIf anyome decides to use this code, a lot more tuning is necessary before the XGBoost is useful for this dataset.\n\nNow, onto the coding:\n\n# Loading the packages\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output-stderr}\n```\n── Attaching packages ────────────────────────────────────── tidymodels 0.2.0 ──\n```\n:::\n\n::: {.cell-output-stderr}\n```\n✔ broom        0.8.0     ✔ recipes      0.2.0\n✔ dials        0.1.1     ✔ rsample      0.1.1\n✔ dplyr        1.0.9     ✔ tibble       3.1.7\n✔ ggplot2      3.3.6     ✔ tidyr        1.2.0\n✔ infer        1.0.0     ✔ tune         0.2.0\n✔ modeldata    0.1.1     ✔ workflows    0.2.6\n✔ parsnip      0.2.1     ✔ workflowsets 0.2.1\n✔ purrr        0.3.4     ✔ yardstick    0.0.9\n```\n:::\n\n::: {.cell-output-stderr}\n```\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output-stderr}\n```\n✔ readr   2.1.2     ✔ forcats 0.5.1\n✔ stringr 1.4.0     \n```\n:::\n\n::: {.cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\n```\n:::\n:::\n\n# Loading the data\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nvb_matches <- read_csv(\"~/Downloads/vb_matches.txt\")\n```\n\n::: {.cell-output-stderr}\n```\nRows: 76756 Columns: 65\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (17): circuit, tournament, country, gender, w_player1, w_p1_country, w_...\ndbl  (42): year, match_num, w_p1_age, w_p1_hgt, w_p2_age, w_p2_hgt, l_p1_age...\ndate  (5): date, w_p1_birthdate, w_p2_birthdate, l_p1_birthdate, l_p2_birthdate\ntime  (1): duration\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nView(vb_matches)\n```\n\n::: {.cell-output-stderr}\n```\nWarning in system2(\"/usr/bin/otool\", c(\"-L\", shQuote(DSO)), stdout = TRUE):\nrunning command ''/usr/bin/otool' -L '/Library/Frameworks/R.framework/Resources/\nmodules/R_de.so'' had status 1\n```\n:::\n:::\n\n## Taking only the top 1000 rows\n\nthe original dataset is \\~80x bigger\n\n::: {.cell}\n\n```{.r .cell-code}\nvb_matches <- vb_matches |> arrange(desc(w_p1_tot_kills)) |> \n  slice_head(n=1000)\n```\n:::\n\n## cleaning the columns, and combining some variables into new \"total\" variables\n\n::: {.cell}\n\n```{.r .cell-code}\nvb_parsed <- vb_matches %>%\n  transmute(\n    circuit,\n    gender,\n    year,\n    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,\n    w_kills = w_p1_tot_kills + w_p2_tot_kills,\n    w_errors = w_p1_tot_errors + w_p2_tot_errors,\n    w_aces = w_p1_tot_aces + w_p2_tot_aces,\n    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,\n    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,\n    w_digs = w_p1_tot_digs + w_p2_tot_digs,\n    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,\n    l_kills = l_p1_tot_kills + l_p2_tot_kills,\n    l_errors = l_p1_tot_errors + l_p2_tot_errors,\n    l_aces = l_p1_tot_aces + l_p2_tot_aces,\n    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,\n    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,\n    l_digs = l_p1_tot_digs + l_p2_tot_digs\n  )  |> \n  na.omit()\n```\n:::\n\n# splitting the data into winners and losers\n\nand further making a dataframe to use as a starting spot.\n\n::: {.cell}\n\n```{.r .cell-code}\nwinners <- vb_parsed %>%\n  select(circuit, gender, year,\n         w_attacks:w_digs) %>%\n  rename_with(~ str_remove_all(., \"w_\"), w_attacks:w_digs) %>%\n  mutate(win = \"win\")\n\nlosers <- vb_parsed %>%\n  select(circuit, gender, year,\n         l_attacks:l_digs) %>%\n  rename_with(~ str_remove_all(., \"l_\"), l_attacks:l_digs) %>%\n  mutate(win = \"lose\")\n\nvb_df <- bind_rows(winners, losers) %>%\n  mutate_if(is.character, factor)\n```\n:::\n\n# Data Viz\n\n::: {.cell}\n\n```{.r .cell-code}\nvb_df %>%\n  pivot_longer(attacks:digs, names_to = \"stat\", values_to = \"value\") %>%\n  ggplot(aes(gender, value, fill = win, color = win)) +\n  geom_boxplot(alpha = 0.4) +\n  facet_wrap(~stat, scales = \"free_y\", nrow = 2) +\n  labs(y = NULL, color = NULL, fill = NULL)\n```\n\n::: {.cell-output-display}\n![](xgboost_files/figure-html/unnamed-chunk-6-1.png){width=768}\n:::\n:::\n\n# Splitting the data into train and test\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nvb_split <- initial_split(vb_df, strata = win)\nvb_train <- training(vb_split)\nvb_test <- testing(vb_split)\n```\n:::\n\n# Spec the XGBoost model with no tuning params\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_spec <- boost_tree(\n  trees = 1000, \n  tree_depth = tune(), min_n = tune(), \n  loss_reduction = tune(),                     ## first three: model complexity\n  sample_size = tune(), mtry = tune(),         ## randomness\n  learn_rate = tune(),                         ## step size\n) %>% \n  set_engine(\"xgboost\") %>% \n  set_mode(\"classification\")\n\nxgb_spec\n```\n\n::: {.cell-output-stdout}\n```\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n:::\n:::\n\n# XGBoost grid\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_grid <- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), vb_train),\n  learn_rate(),\n  size = 30\n)\n\nxgb_grid\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 30 × 6\n   tree_depth min_n loss_reduction sample_size  mtry    learn_rate\n        <int> <int>          <dbl>       <dbl> <int>         <dbl>\n 1          3    14   0.000242           0.129     3 0.000786     \n 2          5     2   0.00862            0.794    11 0.00154      \n 3          1    21   0.0000000171       0.533     2 0.0000188    \n 4         11    27  12.1                0.162     5 0.00000450   \n 5          5    19   0.152              0.580     1 0.0155       \n 6          5    23   0.0000150          0.590     9 0.0356       \n 7          8    17   0.00000433         0.677     5 0.000347     \n 8          6     9   0.630              0.287     8 0.00622      \n 9          1     6   0.000460           0.201     6 0.000000146  \n10          2    36   0.00315            0.625     8 0.00000000168\n# … with 20 more rows\n```\n:::\n:::\n\n# Preparing the workflow formula\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_wf <- workflow() %>%\n  add_formula(win ~ .) %>%\n  add_model(xgb_spec)\n\nxgb_wf\n```\n\n::: {.cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nwin ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 1000\n  min_n = tune()\n  tree_depth = tune()\n  learn_rate = tune()\n  loss_reduction = tune()\n  sample_size = tune()\n\nComputational engine: xgboost \n```\n:::\n:::\n\n# Data folds\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nvb_folds <- vfold_cv(vb_train, strata = win)\n\nvb_folds\n```\n\n::: {.cell-output-stdout}\n```\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [1320/148]> Fold01\n 2 <split [1320/148]> Fold02\n 3 <split [1320/148]> Fold03\n 4 <split [1320/148]> Fold04\n 5 <split [1322/146]> Fold05\n 6 <split [1322/146]> Fold06\n 7 <split [1322/146]> Fold07\n 8 <split [1322/146]> Fold08\n 9 <split [1322/146]> Fold09\n10 <split [1322/146]> Fold10\n```\n:::\n:::\n\n# Running the model\n\n## Note: Takes a long time to process.\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nset.seed(234)\nxgb_res <- tune_grid(\n  xgb_wf,\n  resamples = vb_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nxgb_res\n```\n\n::: {.cell-output-stdout}\n```\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 5\n   splits             id     .metrics           .notes           .predictions\n   <list>             <chr>  <list>             <list>           <list>      \n 1 <split [1320/148]> Fold01 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 2 <split [1320/148]> Fold02 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 3 <split [1320/148]> Fold03 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 4 <split [1320/148]> Fold04 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 5 <split [1322/146]> Fold05 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 6 <split [1322/146]> Fold06 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 7 <split [1322/146]> Fold07 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 8 <split [1322/146]> Fold08 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 9 <split [1322/146]> Fold09 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n10 <split [1322/146]> Fold10 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n```\n:::\n:::\n\n# Results\n\n::: {.cell}\n\n```{.r .cell-code}\ncollect_metrics(xgb_res)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 60 × 12\n    mtry min_n tree_depth learn_rate loss_reduction sample_size .metric \n   <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>   \n 1     3    14          3 0.000786     0.000242           0.129 accuracy\n 2     3    14          3 0.000786     0.000242           0.129 roc_auc \n 3    11     2          5 0.00154      0.00862            0.794 accuracy\n 4    11     2          5 0.00154      0.00862            0.794 roc_auc \n 5     2    21          1 0.0000188    0.0000000171       0.533 accuracy\n 6     2    21          1 0.0000188    0.0000000171       0.533 roc_auc \n 7     5    27         11 0.00000450  12.1                0.162 accuracy\n 8     5    27         11 0.00000450  12.1                0.162 roc_auc \n 9     1    19          5 0.0155       0.152              0.580 accuracy\n10     1    19          5 0.0155       0.152              0.580 roc_auc \n# … with 50 more rows, and 5 more variables: .estimator <chr>, mean <dbl>,\n#   n <int>, std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n# Data Viz results\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, mtry:sample_size) %>%\n  pivot_longer(mtry:sample_size,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n```\n\n::: {.cell-output-display}\n![](xgboost_files/figure-html/unnamed-chunk-14-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshow_best(xgb_res, \"roc_auc\")\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 5 × 12\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .metric\n  <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>  \n1     8     9          6    0.00622   0.630              0.287 roc_auc\n2     6    13          9    0.00204   0.0000000676       0.898 roc_auc\n3    11     2          5    0.00154   0.00862            0.794 roc_auc\n4     1     5         13    0.0639    2.47               0.781 roc_auc\n5     9    23          5    0.0356    0.0000150          0.590 roc_auc\n# … with 5 more variables: .estimator <chr>, mean <dbl>, n <int>,\n#   std_err <dbl>, .config <chr>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbest_auc <- select_best(xgb_res, \"roc_auc\")\nbest_auc\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 1 × 7\n   mtry min_n tree_depth learn_rate loss_reduction sample_size .config          \n  <int> <int>      <int>      <dbl>          <dbl>       <dbl> <chr>            \n1     8     9          6    0.00622          0.630       0.287 Preprocessor1_Mo…\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_xgb <- finalize_workflow(\n  xgb_wf,\n  best_auc\n)\n\nfinal_xgb\n```\n\n::: {.cell-output-stdout}\n```\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nwin ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 8\n  trees = 1000\n  min_n = 9\n  tree_depth = 6\n  learn_rate = 0.00621594835720267\n  loss_reduction = 0.629777153856609\n  sample_size = 0.286838799694087\n\nComputational engine: xgboost \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res <- last_fit(final_xgb, vb_split)\n\ncollect_metrics(final_res)\n```\n\n::: {.cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.7   Preprocessor1_Model1\n2 roc_auc  binary         0.759 Preprocessor1_Model1\n```\n:::\n:::\n\n# ROC Curve\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_res %>%\n  collect_predictions() %>%\n  roc_curve(win, .pred_win) %>%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"midnightblue\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  )\n```\n\n::: {.cell-output-display}\n![](xgboost_files/figure-html/unnamed-chunk-19-1.png){width=768}\n:::\n:::",
    "supporting": [
      "xgboost_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": [],
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}