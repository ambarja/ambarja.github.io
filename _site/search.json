[
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#installation",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#installation",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Installation",
    "text": "Installation\nAssuming you already have R installed on your system, Installing VSCode and R is very straightforward;\n\ndownload the .pkg or .exe file and install the code editor;\ninstall the languageserver package for R (install.packages(“languageserver”)\ninstall the R extension from the markeplace (https://marketplace.visualstudio.com/items?itemName=REditorSupport)\n\nand you’re ready to start coding.\nOptional: You can install a better terminal alternative like radian, a debugger and a better plotviewer like httpgd because the builtin plotviewer for VSCode isn’t a particualrly good one."
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#setting-up-the-editor",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#setting-up-the-editor",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Setting up the editor",
    "text": "Setting up the editor\nYou can now start working with code but there are potentially times you could run into errors while running your code.\nTo eliminate any potential errors/code not work, you must make sure the R path is specified in the VSCode preferences (cmd+shift+p on mac) and search for R path. Finding the path is simple, open R and type: R.home(“bin”) and copy the output path.\nAnother somewhat major tweak you would want to do to the editor:\nSome simple shortcuts like the pipe operator doesn’t work out of the box, so adding a shortcut for “%>%” or “|>” depending on your preference can be done using the following method:\nhttps://stackoverflow.com/questions/58731362/how-do-i-had-an-alias-for-magrittr-pipe-from-r-in-vscode How do I had an alias for magrittr pipe from R in vscode - Stack Overflow"
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#working-with-r-code",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#working-with-r-code",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Working with R code",
    "text": "Working with R code\nHere is a comparison of my two setups side by side with the same file and environment:\n\n\nworking with R is very similar to a regular RStudio IDE,\nR-extension provides the same support for\n\ninstalling packages,\nplots and plot viewer\ngrobal enviroment\n\ndatasets\nplots\nlists, variables, etc.\n\nchecking loaded packages etc.\n\nTerminal and console position can be modified to make get the input and output panes next to each other, something you cannot do well in R-Studio. So VSCode feels very familiar and running code is highly intuitive."
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#running-code",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#running-code",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Running Code",
    "text": "Running Code\nRunning a piece of code is again, very similar to RStudio. A run file button is located at the top of the panel and individual lines of code can be run using the cmd+return shortcut on mac. The shortcut can be easily modified similar to how the pipe operator shortcut is modified above.\nIn terms of running and getting results for plain R code and .R files; VSCode is a great alternative for a lot of people including me. So if you’re like me and you use R mainly for statistical analysis, data visualization and data analysis; give VSCode a try.\nWorking with Notebooks and Rmd:\nThis is where VSCode falls so much behind RStudio it becomes a one horse race. The notebook support in VSCode is in a single word - terrible. Running code chunks in a .Rmd notebook feels and looks dated. My existing notebooks created in RStudio were buggy in VSCode despite not showing any errors in RStudio. Creating new notebooks is unintuitive and still a long way behind RStudio in terms of the overall feel to it.\nI have searched for solutions, including globally installing pandoc, knitr and rmarkdown etc and trying multiple versions of R and the packages mentioned previously to no avail. This is one of the biggest reasons I am not completely switching to VSCode just yet."
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#a-note-about-quarto",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#a-note-about-quarto",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "A Note About Quarto",
    "text": "A Note About Quarto\nI recently started playing around with Quarto, which looks like the natural successor to Rmarkdown. So far, I have only used Quarto in RStudio and am a big fan of the way it looks and works. I have yet to try it on VSCode, so I cannot comment on it."
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#what-i-love-about-r-in-vscode",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#what-i-love-about-r-in-vscode",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "What I love about R in VSCode",
    "text": "What I love about R in VSCode\n\nSpeed - VSCode is quick to load up, fast and snappy when the code is running. I did not do any analysis on the times it took to load a piece of code on one vs the other but VSCode ‘feels’ quicker.\nOne stop shop for R, Python, HTML, CSS, and dozens other languages - and does it much better than RStudio\nMULTIPLE R SESSIONS!!! - Such a great feature when you’re trying to develop several related projects at the same time.\nCode Refactoring - much better than RStudio\nColor picker and color blocksFor people like me who work with a lot of data visualizations, the ability to see the colors and use the color blocks to pick colors seamlessly is a gamechanger.\n\n\n\nLanguage server - document outline - Navigating through a long line of code (and multiple R files) is painless\nIntellisense and Code Completion - faster, more user friendly\n\n\n\nHelp and documentation: Same information, much more streamlined\n\n\n\nLiveshare and working with teams\nCode Snippets are easier to use and set up than RStudio\nMuch easier git and version control integration\nFunctions are handled brilliantly. Hover over them and you get all kinds of information and callbacks.\nCode organization: Code sections allow for folding of code, makes fo easier code reading and naviation and cleaner code overall.\n\n\n\nCustomizable and make it your own by editing the json files inside VSCode.\n\nSo VSCode has some amazing features that could really tempt an RStudio user."
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#what-i-dont-love-about-r-in-vscode",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#what-i-dont-love-about-r-in-vscode",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "What I don’t love about R in VSCode",
    "text": "What I don’t love about R in VSCode\n\nR-studio works great out of the box, no need to install extensions while VSCode setup for R can be tricky and can sometimes involve trail and error\nR-templates inside R studio are amazing, and easy to find and use\nDataviewer in RStudio feels a lot better than VScode, especially when working with a large dataset\nEven though I use VSCode for Python, HTML, CSS etc, coding R in it, especially for the first few hours feels very odd and in a way unfamiliar despite the very familiar overall IDE organization\nNOTEBOOK SUPPORT - If you work with rmd more than r, DO NOT SWITCH. Running chunks of code is very clunky and poor\nKnitting notebooks doesnt work well at all, and can lead to errors a lot of the time.\nPlot viewers are still not as good as RStudio\nTerminal will sometimes throw errors in VSCode that you wouldn’t find for the same piece of code in RStudio\nRStudio just feels a lot more tailored to a data scientist/statistical analysis person, while VSCode can feel like a developer-centric code editor\nFeels a lot more tailored to a data scientist/statistical analysis"
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#things-i-hope-to-try-soon-with-vscode",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#things-i-hope-to-try-soon-with-vscode",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Things I hope to try soon with VScode",
    "text": "Things I hope to try soon with VScode\n\nQuarto\nRblogdown and Rbookdown\nMore Debugging\nRadian terminal"
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#conclusion",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#conclusion",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Conclusion",
    "text": "Conclusion\nDo I love R in VSCode so far? Absolutely\nEnough to make me want to switch? Not yet.\nWhy? Notebooks\nWill I switch soon? Hopefully yes.\n\nIf you liked this, and have any suggestions for me; find me on twitter: https://twitter.com/karat_sidhu"
  },
  {
    "objectID": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#further-reading",
    "href": "posts/vscode-vs-rstudio-worth-the-switch/vscode-vs-rstudio-worth-the-switch.html#further-reading",
    "title": "VSCode vs RStudio — Worth the Switch?",
    "section": "Further Reading",
    "text": "Further Reading\n\nA great video and blog by Kun Ren on the subject : https://youtu.be/9xXBDU2z_8Y\nA blog on the same subject: https://renkun.me/2019/12/11/writing-r-in-vscode-a-fresh-start/\nRunning R in VSCode: https://www.infoworld.com/article/3625488/how-to-run-r-in-visual-studio-code.html\nInsalling R in VSCode: https://code.visualstudio.com/docs/languages/r\nBonus: Quarto vs RMD: https://yihui.org/en/2022/04/quarto-r-markdown/\n\nThis writeup is also available on Towards Data Science\nVSCode vs RStudio Worth the switch? | Karat Sidhu | Towards Data Science (medium.com)"
  },
  {
    "objectID": "posts/epl-2022-final-table/epl-2022-final-table.html",
    "href": "posts/epl-2022-final-table/epl-2022-final-table.html",
    "title": "EPL 2022 Final Table",
    "section": "",
    "text": "Loading the required packages\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nlibrary(RColorBrewer)\n\nThe main font I used for the title is similar to the PL official font.\n\nshowtext_auto()\nsysfonts::font_add(\"Premier League\", regular = \"FontsFree-Net-Barclays-Premier-League.ttf\")\n\nThe data file was downloaded from the FBref database as a .csv, so the loading and cleaning of data was a relatively easy process\n\n\nLoading the data\n\ntable <- read.csv(\"pl_table_gw_38.csv\")\n\n\nlogos <-\n    read.csv(\n        \"https://raw.githubusercontent.com/steodose/Club-Soccer-Forecasts/main/team_mapping.csv\"\n    )\n\nData to be cleaned and the form table needs to be in the form of a list inside a data frame. Cleaning it up:\n\nlogos_cleaned <- logos |>\n    select(squad_fbref, url_logo_espn) |>\n    rename(Squad = \"squad_fbref\")\n\ntable <- table |>\n    select(-c(Goalkeeper, Notes, Attendance))\n\n\n\nCombining the two tables,\n\ntable2 <- inner_join(table,\n    logos_cleaned,\n    by = \"Squad\"\n) |>\n    relocate(Rk, url_logo_espn)\n\n\n\nMaking the final table:\n\nfinal_table <- table2 |>\n    rename(` ` = \"url_logo_espn\") |>\n    rename(Rank = \"Rk\") |>\n    rename(`Top Scorer` = \"Top.Team.Scorer\") |>\n    gt() |>\n    gt_color_rows(\n        columns = W,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$W), min(table2$W))\n    ) |>\n    gt_color_rows(\n        columns = D,\n        type = \"discrete\",\n        direction = -1,\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$D), min(table2$D))\n    ) |>\n    gt_color_rows(\n        columns = L,\n        type = \"discrete\",\n        direction = -1,\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$L), min(table2$L))\n    ) |>\n    gt_color_rows(\n        columns = GF,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$GF), min(table2$GF))\n    ) |>\n    gt_color_rows(\n        columns = GA,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        direction = -1,\n        domain = c(min(table2$GA), max(table2$GA))\n    ) |>\n    gt_color_rows(\n        columns = GD,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$GD), min(table2$GD))\n    ) |>\n    gt_color_rows(\n        columns = Pts,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$Pts), min(table2$Pts))\n    ) |>\n    gt_color_rows(\n        xG,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$xG), min(table2$xG))\n    ) |>\n    gt_color_rows(\n        xGA,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        direction = -1,\n        domain = c(min(table2$xGA), max(table2$xGA))\n    ) |>\n    gt_color_rows(\n        xGD,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$xGD), min(table2$xGD))\n    ) |>\n    gt_color_rows(\n        xGD.90,\n        type = \"discrete\",\n        palette = \"RColorBrewer::Spectral\",\n        domain = c(max(table2$xGD.90), min(table2$xGD.90))\n    ) |>\n    gt_img_rows(\n        columns = ` `,\n        img_source = \"web\",\n        height = 35\n    ) |>\n    tab_style(\n        style = list(cell_text(weight = \"bold\")),\n        locations = cells_body(rows = Squad == \"Manchester Utd\")\n    ) |>\n    gt_theme_espn() |>\n    tab_style(\n        style = list(cell_text(weight = \"bold\")),\n        locations = cells_body(\n            columns = Squad\n        )\n    ) |>\n    tab_header(\n        title = \"2021-22 Premier League Standings\",\n        subtitle = \"Final Results for the English Premier League saw Manchester City win the league after an exciting last few weeks of competition\n    with Liverpool. The competition came down to the final few minutes of the season where City were crowned winners.\n    The top 4 teams qualified for the UCL, 5th and 6th place teams are in the Europa League while the 7th place West Ham\n    secured the UECL qualification. The bottom 3 teams, Norwich, Watford and Burnley were relegated to the English Championship (2nd tier competition);\n    Everton and Leeds United narrowly escaped relegation.\"\n    ) |>\n    tab_footnote(\n        footnote = \"xG = Expected Goals (Higher is better)\",\n        locations = cells_column_labels(columns = xG)\n    ) |>\n    tab_footnote(\n        footnote = \"xGA = Expect Goals Against (Lower is better) \",\n        locations = cells_column_labels(columns = xGA)\n    ) |>\n    tab_footnote(\n        footnote = \"xGD = xG Difference (xG - xGA (High is better))\",\n        locations = cells_column_labels(columns = xGD)\n    ) |>\n    tab_footnote(\n        footnote = \"xGD.90 = xG Difference per 90 mins (Higher is better)\",\n        locations = cells_column_labels(columns = xGD.90)\n    ) |>\n    tab_footnote(\n        footnote = \"The team that I support :(\",\n        locations = cells_body(\n            columns = Squad,\n            rows = 6\n        )\n    ) |>\n    tab_source_note(\n        source_note = \"Data : FBRef | Table : Github.com/SidhuK\"\n    ) |>\n    tab_style(\n        locations = cells_title(groups = \"title\"), # format the main title\n        style = list(\n            cell_text(\n                font = \"Premier League\",\n                size = px(40),\n                color = \"black\",\n                weight = 700\n            )\n        )\n    ) |>\n    tab_options(\n        table.background.color = \"#eeeeee\",\n        column_labels.background.color = \"#eeeeee\"\n    ) # set the bg color\n\n\n\nSaving the table as png (optional)\n(Use gt save to save it as a png and not an html object)\n\n\nFinal Result:\n\nfinal_table\n\nWarning in locname == \"data\" && colname == .env$colname: 'length(x) = 245 > 1'\nin coercion to 'logical(1)'\n\nWarning in locname == \"data\" && colname == .env$colname: 'length(x) = 245 > 1'\nin coercion to 'logical(1)'\n\n\n\n\n\n\n  \n    \n      2021-22 Premier League Standings\n    \n    \n      Final Results for the English Premier League saw Manchester City win the league after an exciting last few weeks of competition\n    with Liverpool. The competition came down to the final few minutes of the season where City were crowned winners.\n    The top 4 teams qualified for the UCL, 5th and 6th place teams are in the Europa League while the 7th place West Ham\n    secured the UECL qualification. The bottom 3 teams, Norwich, Watford and Burnley were relegated to the English Championship (2nd tier competition);\n    Everton and Leeds United narrowly escaped relegation.\n    \n  \n  \n    \n      Rank\n       \n      Squad\n      MP\n      W\n      D\n      L\n      GF\n      GA\n      GD\n      Pts\n      xG1\n      xGA2\n      xGD3\n      xGD.904\n      Top Scorer\n    \n  \n  \n    1\n\nManchester City\n38\n29\n6\n3\n99\n26\n73\n93\n86.1\n26.9\n59.3\n1.60\nKevin De Bruyne - 15\n    2\n\nLiverpool\n38\n28\n8\n2\n94\n26\n68\n92\n86.1\n33.1\n52.9\n1.43\nMohamed Salah - 23\n    3\n\nChelsea\n38\n21\n11\n6\n76\n33\n43\n74\n64.8\n34.9\n29.9\n0.81\nMason Mount - 11\n    4\n\nTottenham\n38\n22\n5\n11\n69\n40\n29\n71\n61.5\n39.0\n22.5\n0.61\nSon Heung-min - 23\n    5\n\nArsenal\n38\n22\n3\n13\n61\n48\n13\n69\n56.8\n46.0\n10.8\n0.29\nBukayo Saka - 11\n    6\n\nManchester Utd5\n38\n16\n10\n12\n57\n57\n0\n58\n54.0\n54.1\n-0.1\n0.00\nCristiano Ronaldo - 18\n    7\n\nWest Ham\n38\n16\n8\n14\n60\n51\n9\n56\n49.6\n49.3\n0.3\n0.01\nJarrod Bowen - 12\n    8\n\nLeicester City\n38\n14\n10\n14\n62\n59\n3\n52\n48.5\n59.8\n-11.3\n-0.30\nJamie Vardy - 15\n    9\n\nBrighton\n38\n12\n15\n11\n42\n44\n-2\n51\n44.4\n45.2\n-0.9\n-0.02\nLeandro Trossard Neal Maupay - 8\n    10\n\nWolves\n38\n15\n6\n17\n38\n43\n-5\n51\n34.9\n57.1\n-22.3\n-0.60\nRaúl Jiménez - 6\n    11\n\nNewcastle Utd\n38\n13\n10\n15\n44\n62\n-18\n49\n38.5\n54.0\n-15.5\n-0.42\nCallum Wilson - 8\n    12\n\nCrystal Palace\n38\n11\n15\n12\n50\n46\n4\n48\n45.9\n40.1\n5.7\n0.16\nWilfried Zaha - 14\n    13\n\nBrentford\n38\n13\n7\n18\n48\n56\n-8\n46\n46.7\n47.7\n-1.0\n-0.03\nIvan Toney - 12\n    14\n\nAston Villa\n38\n13\n6\n19\n52\n54\n-2\n45\n43.7\n46.2\n-2.5\n-0.07\nOllie Watkins - 11\n    15\n\nSouthampton\n38\n9\n13\n16\n43\n67\n-24\n40\n45.6\n56.3\n-10.7\n-0.29\nJames Ward-Prowse - 10\n    16\n\nEverton\n38\n11\n6\n21\n43\n66\n-23\n39\n41.6\n51.4\n-9.8\n-0.26\nRicharlison - 10\n    17\n\nLeeds United\n38\n9\n11\n18\n42\n79\n-37\n38\n45.8\n69.0\n-23.2\n-0.63\nRaphael Dias Belloli - 11\n    18\n\nBurnley\n38\n7\n14\n17\n34\n53\n-19\n35\n37.5\n54.9\n-17.4\n-0.47\nMaxwel Cornet - 9\n    19\n\nWatford\n38\n6\n5\n27\n34\n77\n-43\n23\n37.8\n64.2\n-26.4\n-0.71\nEmmanuel Dennis - 10\n    20\n\nNorwich City\n38\n5\n7\n26\n23\n84\n-61\n22\n33.0\n73.3\n-40.3\n-1.09\nTeemu Pukki - 11\n  \n  \n    \n      Data : FBRef | Table : Github.com/SidhuK\n    \n  \n  \n    \n      1 xG = Expected Goals (Higher is better)\n    \n    \n      2 xGA = Expect Goals Against (Lower is better) \n    \n    \n      3 xGD = xG Difference (xG - xGA (High is better))\n    \n    \n      4 xGD.90 = xG Difference per 90 mins (Higher is better)\n    \n    \n      5 The team that I support :(\n    \n  \n\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{sidhu2022,\n  author = {Karat Sidhu},\n  editor = {},\n  title = {EPL 2022 {Final} {Table}},\n  date = {2022-05-23},\n  url = {https://karatsidhu.com/posts/epl-2022-final-table/epl-2022-final-table.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKarat Sidhu. 2022. “EPL 2022 Final Table.” May 23, 2022. https://karatsidhu.com/posts/epl-2022-final-table/epl-2022-final-table.html."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html",
    "title": "TidyModels - SVM & Random Forests",
    "section": "",
    "text": "The purpose of this post is for me to learn more about tidymodels package, as well as learning and deploying models for prediction. This is hopefully a first in the series of many posts where I try and learn more about various algorithms that are present in this package."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#goal",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#goal",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Goal",
    "text": "Goal\nThe goal of this first exercise is to learn more about text analysis and using various reviews from chocolates to predict the ratings for a particular chocolate bar.\nThis is a very vague and non specific way of predicting the outcome but a good starting point in learning how the algorithms work. ## Loading the packages\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(tidytext)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 0.2.0 ──\n\n\n✔ broom        0.8.0     ✔ rsample      0.1.1\n✔ dials        0.1.1     ✔ tune         0.2.0\n✔ infer        1.0.0     ✔ workflows    0.2.6\n✔ modeldata    0.1.1     ✔ workflowsets 0.2.1\n✔ parsnip      0.2.1     ✔ yardstick    0.0.9\n✔ recipes      0.2.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(textrecipes)"
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#loading-the-data",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#loading-the-data",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Loading the data",
    "text": "Loading the data\n\nchocolate <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv')\n\nRows: 2530 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): company_manufacturer, company_location, country_of_bean_origin, spe...\ndbl (3): ref, review_date, rating\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#lets-see-how-the-ratings-are-distributed",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#lets-see-how-the-ratings-are-distributed",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Lets see how the ratings are distributed",
    "text": "Lets see how the ratings are distributed\n\nchocolate |> \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 12) +\n  theme_minimal() +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nFrom the chart above, it looks like that most chocolates are rated somewhere in the range of 2.5 and 3.75 with a few high-rated and low-rated exceptions.\nSo we’ll be comparing the ratings and the descriptive words used to describe the corresponding ratings."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#tidytext-words-analysis",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#tidytext-words-analysis",
    "title": "TidyModels - SVM & Random Forests",
    "section": "TidyText words analysis",
    "text": "TidyText words analysis\nLets use the tidytext() library to check what are some of the most common words used to describe the flavor of each chocolate in the data-set.\n\n# split the characteristics column into words using tidytext, and make a new column called word instead of the original.\n\ntidy_chocolate <-\n  chocolate %>%\n  unnest_tokens(word, most_memorable_characteristics)\n\ntidy_chocolate |> \n  group_by(word) |> \n  summarise(total = n()) |> arrange(desc(total))\n\n# A tibble: 547 × 2\n   word    total\n   <chr>   <int>\n 1 cocoa     419\n 2 sweet     318\n 3 nutty     278\n 4 fruit     273\n 5 roasty    228\n 6 mild      226\n 7 sour      208\n 8 earthy    199\n 9 creamy    189\n10 intense   178\n# … with 537 more rows\n\n\nIt looks like the usual expected words like cocoa, sweet, nutty etc are the most prevalent.\nSince we know what the most common words are, its time to look at how an average chocolate described by these words is rated.\n\nRating vs Word relationship\n\ntidy_chocolate |> \n  group_by(word) |> \n  summarise(\n    n = n(),\n    rating = mean(rating) \n  ) |> \n  ggplot(aes(n, rating)) +\n  geom_jitter(color = \"maroon\", alpha = 0.7) +\n   geom_hline(\n    yintercept = mean(chocolate$rating), lty = 2,\n    color = \"gray50\", size = 1.5\n  ) + ggrepel::geom_text_repel(aes(label = word), max.overlaps = 15) +\n  scale_x_log10() +\n  theme_minimal() +\n  hrbrthemes::theme_ipsum()\n\nWarning: ggrepel: 495 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nWords like chemical, burnt, medicinal, pastey, bitter etc look to be associated with generally low rated chocolate, while cocoa, complex, creamy, balanced etc are higher rated chocolates.3\nWe now have a bit of an idea of what the general feeling of the data rating, and we can go on to buiding the models."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#building-models-with-tidymodels",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#building-models-with-tidymodels",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Building Models with Tidymodels",
    "text": "Building Models with Tidymodels\nLet’s start our modeling by setting up our “data budget.” We’ll stratify by our outcome “rating” which is what we want to measure using the tokens.\n\nlibrary(tidymodels)\n\nTime to split the data into training and testing data\n\nset.seed(123)\nchoco_split <- initial_split(chocolate, strata = rating)\nchoco_train <- training(choco_split)\nchoco_test <- testing(choco_split)\n\n\ncreate resampling folds from the training set\n\n\nset.seed(234)\nchoco_folds <- vfold_cv(choco_train, strata = rating)\nchoco_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits             id    \n   <list>             <chr> \n 1 <split [1705/191]> Fold01\n 2 <split [1705/191]> Fold02\n 3 <split [1705/191]> Fold03\n 4 <split [1706/190]> Fold04\n 5 <split [1706/190]> Fold05\n 6 <split [1706/190]> Fold06\n 7 <split [1707/189]> Fold07\n 8 <split [1707/189]> Fold08\n 9 <split [1708/188]> Fold09\n10 <split [1709/187]> Fold10\n\n\nWe’re done with splitting the data into test and train, and we’re using the training data to train the model. So the first step will involve setting up feature engineering. The data right now is complex and we need to transform it into features that are useful for our model tokenization and computing."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#tokenization",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#tokenization",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Tokenization",
    "text": "Tokenization\n(if that’s a word?)\nWe’ll use textrecipes package to tokenize “most_memorable_characteristics” wrt “ratings” and look at the 100 most common words used (here they are called tokens).All of this is done on the\n\nlibrary(textrecipes)\n\nchoco_rec <-\n  recipe(rating ~ most_memorable_characteristics, data = choco_train) %>%\n  step_tokenize(most_memorable_characteristics) %>%\n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% # 100 most common words\n  step_tfidf(most_memorable_characteristics) # step frequeence df\n\n\n# looking at the tokenized data\nprep(choco_rec) %>% bake(new_data = NULL)\n\n# A tibble: 1,896 × 101\n   rating tfidf_most_memorab… tfidf_most_memo… tfidf_most_memo… tfidf_most_memo…\n    <dbl>               <dbl>            <dbl>            <dbl>            <dbl>\n 1   3                   0                   0                0                0\n 2   2.75                0                   0                0                0\n 3   3                   0                   0                0                0\n 4   3                   0                   0                0                0\n 5   2.75                0                   0                0                0\n 6   3                   1.38                0                0                0\n 7   2.75                0                   0                0                0\n 8   2.5                 0                   0                0                0\n 9   2.75                0                   0                0                0\n10   3                   0                   0                0                0\n# … with 1,886 more rows, and 96 more variables:\n#   tfidf_most_memorable_characteristics_base <dbl>,\n#   tfidf_most_memorable_characteristics_basic <dbl>,\n#   tfidf_most_memorable_characteristics_berry <dbl>,\n#   tfidf_most_memorable_characteristics_bitter <dbl>,\n#   tfidf_most_memorable_characteristics_black <dbl>,\n#   tfidf_most_memorable_characteristics_bland <dbl>, …\n\n\nThe result is basically a new dataframe from the “choco_train” data with all the ratings, and the frequency of corresponding 100 most common words."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#model-specification",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#model-specification",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Model Specification",
    "text": "Model Specification\nThe models being used to evaluate the data are random forest, and support vector machine (SVM).\nRandom Forest Model is usually not great for text based or language data4 and SVM normally a good model to use for such data, so we’ll be trying both.\n\nRandom Forest Model\n\nSpecifying the RF model\nComputational engine: ranger (default)\n\n### Computational engine: ranger\n\nrf_spec <-\n  rand_forest(trees = 500) %>%\n  set_mode(\"regression\")\n\nrf_spec\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  trees = 500\n\nComputational engine: ranger \n\n\n\n\n\nSVM Model\n\nModel Specification\nComputational engine: LiblineaR (default)\n\nsvm_spec <-\n  svm_linear() %>%\n  set_mode(\"regression\")\n\nsvm_spec\n\nLinear Support Vector Machine Specification (regression)\n\nComputational engine: LiblineaR \n\n\nThe models have been specified and now we can run each of them in our workflow().\nNote: The SVM requires the predictors to all be on the same scale5, but all our predictors are now tf-idf values so we should be pretty much fine.\n\nsvm_wf <- workflow(choco_rec, svm_spec)\nrf_wf <- workflow(choco_rec, rf_spec)\n\nWe are done with making the models and now can evaluate both of them."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#how-did-these-two-models-compare",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#how-did-these-two-models-compare",
    "title": "TidyModels - SVM & Random Forests",
    "section": "How did these two models compare?",
    "text": "How did these two models compare?\n\nSVM\n\ncollect_metrics(svm_rs)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.347    10 0.00656 Preprocessor1_Model1\n2 rsq     standard   0.367    10 0.0181  Preprocessor1_Model1\n\n\n\n\nRandom Forest\n\ncollect_metrics(ranger_rs)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   0.350    10 0.00688 Preprocessor1_Model1\n2 rsq     standard   0.359    10 0.0164  Preprocessor1_Model1\n\n\nWe can visualize these results by comparing the predicted rating with the true rating:\n\nbind_rows(\n  collect_predictions(svm_rs) %>%\n    mutate(mod = \"SVM\"),\n  collect_predictions(ranger_rs) %>%\n    mutate(mod = \"ranger\")\n) %>%\n  ggplot(aes(rating, .pred, color = id)) +\n  geom_abline(lty = 2, color = \"gray50\", size = 1.2) +\n  geom_jitter(width = 0.4, alpha = 0.4) +\n  facet_wrap(vars(mod)) +\n  coord_fixed() + hrbrthemes::theme_ipsum()"
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#choosing-a-model",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#choosing-a-model",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Choosing a model",
    "text": "Choosing a model\nNeither of these prediction models look great, judging by their rsq values and the general prediction. However, we can probably use the SVM model for further analysis since it doesn’t take as long as the RF model. The function last_fit() fits one final time on the training data and evaluates on the testing data.\nThis is the first time we have used the testing data.\n\nfinal_fitted <- last_fit(svm_wf, choco_split)\ncollect_metrics(final_fitted) ## metrics evaluated on the *testing* data\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.381 Preprocessor1_Model1\n2 rsq     standard       0.348 Preprocessor1_Model1\n\n\nAgain the results don’t look particularly great, but its just a practise run.\nNow the “final_fitted” object can be used to predict the ratings for everything in the testing data.\nThis is done by using the workflow to predict the choco_test data.\n\nfinal_wf <- extract_workflow(final_fitted)\npredict(final_wf, choco_test[55, ])\n\n# A tibble: 1 × 1\n  .pred\n  <dbl>\n1  3.51\n\n\nNote: You can save this fitted final_wf object to use later with new data, for example with readr::write_rds()."
  },
  {
    "objectID": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#rating-bais-visualization",
    "href": "posts/tidymodels-svm-random-forests/tidymodels-svm-random-forests.html#rating-bais-visualization",
    "title": "TidyModels - SVM & Random Forests",
    "section": "Rating Bais Visualization",
    "text": "Rating Bais Visualization\nWe can now directly visualize the baises for each of the ‘token’ or term and how they affect the rating of a particular chocolate. This is done from the final_fitted object\n\nextract_workflow(final_fitted) %>%\n  tidy() %>% # make a table\n  filter(term != \"Bias\") %>% # remove biases\n  group_by(estimate > 0) %>%\n  slice_max(abs(estimate), n = 10) %>% \n  ungroup() %>%\n  mutate(term = str_remove(term, \"tfidf_most_memorable_characteristics_\")) %>%\n  ggplot(aes(estimate, fct_reorder(term, estimate), fill = estimate > 0)) +\n  geom_col(alpha = 0.8) +\n  scale_fill_discrete(labels = c(\"low ratings\", \"high ratings\")) +\n  labs(y = NULL, fill = \"More from...\") +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nWe see what we noticed during our EDA, i.e. the words like off, bitter, chemical heavily turn the rating negative/low while words like creamy, cocoa, complex etc. tend to be associated with higher rated chocolates."
  },
  {
    "objectID": "posts/quarto-tutorial/quarto-tutorial.html",
    "href": "posts/quarto-tutorial/quarto-tutorial.html",
    "title": "A Quick Guide to Formatting with Quarto",
    "section": "",
    "text": "I decided to start working with Quarto for majority of my writing needs and have been experimenting with its various features, formatting and quirks. I am by no means anywhere close to an expert, however, with this write-up I hope to help out anyone starting out working with QMD files. This is a few small tips and notes (mostly for me) to use as a reference.\nQMD works similar to markdown so basic markdown syntax1 applies for formatting text.\n\n\n\nA lot of Quarto users are coming from RMarkdown, so formatting locations feel familiar and different at the same time. Code chunks format locations are inside the cells preceded with a #|, at the beginning of each code block.\nA simple example of this behavior:\n```{r}\n#| code-fold: true\n#| label: fig-mtcars\n#| fig-cap: \"Fig. 1 Mpg and hp  of Cars\"\n\n\nlibrary(ggplot2)\nggplot(data = mtcars, aes(x = mpg, y=hp)) +\ngeom_smooth() +\ngeom_jitter()\n\n```\nwhich generates the following output:\n\n\nCode\nlibrary(ggplot2)\nggplot(data = mtcars, aes(x = mpg, y=hp)) +\ngeom_smooth() +\ngeom_jitter()\n\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\nFigure 1: Mpg and hp of Cars\n\n\n\n\nFigure caption, code folding are applied from the commands in the first line of the code block.\n\n\n\nIn certain situations, be it for aesthetics or to maximize the use of whitespace, Quarto allows the user to place content including figures, tables, captions etc on the “margin” area.\nThe syntax to add a figure to the margin is:\n\n::: {.column-margin}\n![A margin image](image.png)\n:::\n\n\n\nmtcars[1:6, 1:3]  |> \ngt::gt()  |> gtExtras::gt_theme_538()\n\n\n\n\n\n\n  \n  \n    \n      mpg\n      cyl\n      disp\n    \n  \n  \n    21.0\n6\n160\n    21.0\n6\n160\n    22.8\n4\n108\n    21.4\n6\n258\n    18.7\n8\n360\n    18.1\n6\n225\n  \n  \n  \n\n\n\n\nlibrary(ggplot2)\nggplot(data = mtcars, aes(x = mpg, y=hp)) +\ngeom_smooth() +\ngeom_jitter()\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n\nFigure 2: Mpg and hp of Cars\n\n\n\nAdditionally, captions, comments and text blocks can be added to the margin area in the output.\n\n\n\n\nTo extend the functionality, qmd files also allow us to ignore margins altogether by adding column:screen function to the code chunk. This can result in great looking articles and blogs by adding functions.\nUsing screen fill function, here is a map of NYC embedded into the blog.\n\nlibrary(leaflet)\nleaflet() %>%\n  addTiles() %>%  # Add default OpenStreetMap map tiles\n  addMarkers(lat=40.7128, lng=-74.0060, popup=\"NY City\")\n\n\n\n\n\nFurther, content can be filled partially on the screen.\n\n::: {..column-screen-left}\n:::\n\n\nknitr::kable(\n  mtcars[1:6, 1:10]\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\n\n\n\n\nMazda RX4\n21.0\n6\n160\n110\n3.90\n2.620\n16.46\n0\n1\n4\n\n\nMazda RX4 Wag\n21.0\n6\n160\n110\n3.90\n2.875\n17.02\n0\n1\n4\n\n\nDatsun 710\n22.8\n4\n108\n93\n3.85\n2.320\n18.61\n1\n1\n4\n\n\nHornet 4 Drive\n21.4\n6\n258\n110\n3.08\n3.215\n19.44\n1\n0\n3\n\n\nHornet Sportabout\n18.7\n8\n360\n175\n3.15\n3.440\n17.02\n0\n0\n3\n\n\nValiant\n18.1\n6\n225\n105\n2.76\n3.460\n20.22\n1\n0\n3\n\n\n\n\n\n\n\n\n\nTo draw attention towards something in the text, a block of text call be used as a callout blog in Quarto.\nInline code\n:::{.callout-note}\nNote: This is a callout of type `note`\n:::\nrenders the following output\n\n\n\n\n\n\nNote\n\n\n\nNote: This is a callout of type note\n\n\nOther types of callouts like warning, tip and important are also built in for multiple types of callout blocks.\n\n\n\n\n\n\nTip With Caption\n\n\n\nThis is an example of a callout with a caption.\n\n\n\n\n\n\nOfficial Documents: https://quarto.org/docs/guide/\n\n\n\n\nAcknowledgments section can be added to the end of article/text by using the .appendix command. It will appear at the end of the article/blog with footnotes and citations,etc. formatted as a smaller text font.\n\n## Acknowledgments {.appendix}\n\nThis is the sample text for the acknowledgement section. I made use of the Official Quarto documentation to help me with writing this post."
  },
  {
    "objectID": "posts/quarto-tutorial/quarto-tutorial.html#acknowledgments-1",
    "href": "posts/quarto-tutorial/quarto-tutorial.html#acknowledgments-1",
    "title": "A Quick Guide to Formatting with Quarto",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThis is the sample text for the acknowledgement section. I made use of the Official Quarto documentation to help me with writing this post."
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#column-names",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#column-names",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Column Names",
    "text": "Column Names\nbestsellers %>% colnames()\n## [1] \"Name\"        \"Author\"      \"User.Rating\" \"Reviews\"     \"Price\"      \n## [6] \"Year\"        \"Genre\""
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#checking-for-missing-values",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#checking-for-missing-values",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Checking for missing Values",
    "text": "Checking for missing Values\nany(is.na(bestsellers))\n## [1] FALSE"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#data-at-a-glance",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#data-at-a-glance",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Data at a glance",
    "text": "Data at a glance\nunique(bestsellers$Year)\n##  [1] 2016 2011 2018 2017 2019 2014 2010 2009 2015 2013 2012\nunique(bestsellers$Genre)\n## [1] \"Non Fiction\" \"Fiction\""
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#data-distribution",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#data-distribution",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Data Distribution",
    "text": "Data Distribution\nskim(bestsellers)\n\n\n\nName\nbestsellers\n\n\nNumber of rows\n550\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nData summary\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nName\n0\n1\n4\n121\n0\n351\n0\n\n\nAuthor\n0\n1\n2\n34\n0\n248\n0\n\n\nGenre\n0\n1\n7\n11\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nUser.Rating\n0\n1\n4.62\n0.23\n3.3\n4.5\n4.7\n4.80\n4.9\n▁▁▁▂▇\n\n\nReviews\n0\n1\n11953.28\n11731.13\n37.0\n4058.0\n8580.0\n17253.25\n87841.0\n▇▂▁▁▁\n\n\nPrice\n0\n1\n13.10\n10.84\n0.0\n7.0\n11.0\n16.00\n105.0\n▇▁▁▁▁\n\n\nYear\n0\n1\n2014.00\n3.17\n2009.0\n2011.0\n2014.0\n2017.00\n2019.0\n▇▅▅▅▅"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#all-data-formats-look-good-no-conversion-needed-for-now",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#all-data-formats-look-good-no-conversion-needed-for-now",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "all data formats look good, no conversion needed for now",
    "text": "all data formats look good, no conversion needed for now"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#bestsellers-distribution-by-genre",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#bestsellers-distribution-by-genre",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Bestsellers distribution by genre",
    "text": "Bestsellers distribution by genre\nbestsellers %>%\n  group_by(Genre) %>%\n  summarise(books = n()) %>%\n  ggplot(aes(x = \"\", y = books, fill = Genre)) +\n  geom_bar(stat = \"identity\",\n           width = 1,\n           color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  scale_fill_manual(values = c(\"#ff9900\", \"#000000\"))"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#bestsellers-distribution-by-genre-by-year",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#bestsellers-distribution-by-genre-by-year",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Bestsellers distribution by genre by year",
    "text": "Bestsellers distribution by genre by year\nbestsellers %>%\n  group_by(Year, Genre) %>%\n  summarise(sum = n()) %>%\n  ggplot(aes(\n    x = Year,\n    y = sum,\n    fill = Genre\n  )) +\n  geom_col() +\n  theme_minimal() +\n  scale_x_continuous(breaks = seq(2009, 2019, by = 1)) +\n  scale_fill_manual(values = c(\"#ff9900\", \"#000000\")) +\n  theme(\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor = element_blank(),\n    axis.title.y = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )\n## `summarise()` has grouped output by 'Year'. You can override using the\n## `.groups` argument."
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#is-there-any-correlation-seen-between-the-different-numerical-variables",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#is-there-any-correlation-seen-between-the-different-numerical-variables",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Is there any correlation seen between the different numerical variables?",
    "text": "Is there any correlation seen between the different numerical variables?\nbestsellers %>%\n  select(User.Rating, Reviews, Price, Year) %>%\n  cor() %>%\n  corrplot::corrplot(\n    type = \"lower\",\n    order = \"hclust\",\n    method = \"color\",\n    addgrid.col = \"darkgray\",\n    outline = T,\n    tl.cex = 1,\n    tl.col = \"black\",\n    col = brewer.pal(n = 6, name = \"RdGy\")\n  )"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#most-reviewed-books",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#most-reviewed-books",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Most reviewed books",
    "text": "Most reviewed books\nbestsellers %>% \n  select(Name, Reviews) %>% \n  arrange(desc(Reviews)) %>% \n  head(20) %>% \n  distinct()\n##                                                                                        Name\n## 1                                                                   Where the Crawdads Sing\n## 2                                                                     The Girl on the Train\n## 3                                                                                  Becoming\n## 4                                                                                 Gone Girl\n## 5                                                                    The Fault in Our Stars\n## 6                                                                  The Nightingale: A Novel\n## 7  Fifty Shades of Grey: Book One of the Fifty Shades Trilogy (Fifty Shades of Grey Series)\n## 8                                                                               The Martian\n## 9                                                               All the Light We Cannot See\n## 10                                                                            The Alchemist\n##    Reviews\n## 1    87841\n## 2    79446\n## 3    61133\n## 4    57271\n## 5    50482\n## 6    49288\n## 7    47265\n## 8    39459\n## 9    36348\n## 10   35799"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#are-the-number-of-reviews-changing-by-year",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#are-the-number-of-reviews-changing-by-year",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Are the number of reviews changing by year?",
    "text": "Are the number of reviews changing by year?\n\nLinear Model\nbestsellers %>% \n  lm(formula = Year ~ Reviews) %>% \n  summary()\n## \n## Call:\n## lm(formula = Year ~ Reviews, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5523 -2.5991 -0.3255  2.5354  5.6549 \n## \n## Coefficients:\n##              Estimate Std. Error   t value Pr(>|t|)    \n## (Intercept) 2.013e+03  1.861e-01 10816.121  < 2e-16 ***\n## Reviews     7.111e-05  1.112e-05     6.396 3.42e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.056 on 548 degrees of freedom\n## Multiple R-squared:  0.06946,    Adjusted R-squared:  0.06777 \n## F-statistic: 40.91 on 1 and 548 DF,  p-value: 3.423e-10\n\n\nData Viz\nbestsellers %>% \n  ggplot(aes(Year, Reviews, color = Genre))+\n geom_jitter()+\n geom_smooth(method = 'lm', formula = y ~ x )+\n  theme_minimal() +\n    scale_color_manual(values = c(\"#ff9900\", \"#000000\")) +\n    scale_x_continuous(breaks = seq(2009, 2019, by = 1))+\n   theme(\n    panel.grid.minor = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#highest-rated-books",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#highest-rated-books",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Highest Rated Books",
    "text": "Highest Rated Books\nbestsellers %>% \n  select(Name, User.Rating) %>% \n  arrange(desc(User.Rating)) %>% \n  head(20) %>% \n  distinct()\n##                                                                                               Name\n## 1                                                         Brown Bear, Brown Bear, What Do You See?\n## 2                         Dog Man and Cat Kid: From the Creator of Captain Underpants (Dog Man #4)\n## 3              Dog Man: A Tale of Two Kitties: From the Creator of Captain Underpants (Dog Man #3)\n## 4                  Dog Man: Brawl of the Wild: From the Creator of Captain Underpants (Dog Man #6)\n## 5                           Dog Man: Fetch-22: From the Creator of Captain Underpants (Dog Man #8)\n## 6            Dog Man: For Whom the Ball Rolls: From the Creator of Captain Underpants (Dog Man #7)\n## 7                  Dog Man: Lord of the Fleas: From the Creator of Captain Underpants (Dog Man #5)\n## 8  Goodnight, Goodnight Construction Site (Hardcover Books for Toddlers, Preschool Books for Kids)\n## 9                                                                         Hamilton: The Revolution\n## 10         Harry Potter and the Chamber of Secrets: The Illustrated Edition (Harry Potter, Book 2)\n## 11         Harry Potter and the Goblet of Fire: The Illustrated Edition (Harry Potter, Book 4) (4)\n## 12        Harry Potter and the Prisoner of Azkaban: The Illustrated Edition (Harry Potter, Book 3)\n## 13           Harry Potter and the Sorcerer's Stone: The Illustrated Edition (Harry Potter, Book 1)\n## 14                                                                    Humans of New York : Stories\n## 15                       Jesus Calling: Enjoying Peace in His Presence (with Scripture References)\n##    User.Rating\n## 1          4.9\n## 2          4.9\n## 3          4.9\n## 4          4.9\n## 5          4.9\n## 6          4.9\n## 7          4.9\n## 8          4.9\n## 9          4.9\n## 10         4.9\n## 11         4.9\n## 12         4.9\n## 13         4.9\n## 14         4.9\n## 15         4.9"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#are-users-rating-the-bestsellers-differently-by-year",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#are-users-rating-the-bestsellers-differently-by-year",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Are users rating the bestsellers differently by year?",
    "text": "Are users rating the bestsellers differently by year?\n\nLinear Model\nbestsellers %>% \n  lm(formula = Year ~ Reviews) %>% \n  summary()\n## \n## Call:\n## lm(formula = Year ~ Reviews, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5523 -2.5991 -0.3255  2.5354  5.6549 \n## \n## Coefficients:\n##              Estimate Std. Error   t value Pr(>|t|)    \n## (Intercept) 2.013e+03  1.861e-01 10816.121  < 2e-16 ***\n## Reviews     7.111e-05  1.112e-05     6.396 3.42e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.056 on 548 degrees of freedom\n## Multiple R-squared:  0.06946,    Adjusted R-squared:  0.06777 \n## F-statistic: 40.91 on 1 and 548 DF,  p-value: 3.423e-10\n\n\nData Viz\nbestsellers %>% \n  ggplot(aes(Year, User.Rating, color = Genre))+\n geom_jitter()+\n geom_smooth(method = 'lm', formula = y ~ x )+\n  theme_minimal() +\n    scale_color_manual(values = c(\"#ff9900\", \"#000000\")) +\n    scale_x_continuous(breaks = seq(2009, 2019, by = 1))+\n   theme(\n    panel.grid.minor = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#is-there-a-substantial-change-in-price-over-the-years",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#is-there-a-substantial-change-in-price-over-the-years",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Is there a substantial change in price over the years?",
    "text": "Is there a substantial change in price over the years?\n\nLinear Model\nbestsellers %>% \n  lm(formula = Year ~ Price) %>% \n  summary()\n## \n## Call:\n## lm(formula = Year ~ Price, data = .)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -5.5439 -2.6449 -0.0944  2.7708  5.6248 \n## \n## Coefficients:\n##               Estimate Std. Error  t value Pr(>|t|)    \n## (Intercept) 2014.58885    0.20945 9618.334  < 2e-16 ***\n## Price         -0.04495    0.01232   -3.648 0.000289 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 3.13 on 548 degrees of freedom\n## Multiple R-squared:  0.02371,    Adjusted R-squared:  0.02193 \n## F-statistic: 13.31 on 1 and 548 DF,  p-value: 0.0002895\nbestsellers %>% \n  ggplot(aes(Year, Price, color = Genre))+\n geom_jitter()+\n geom_smooth(method = 'lm', formula = y ~ x )+\n  theme_minimal() +\n    scale_color_manual(values = c(\"#ff9900\", \"#000000\")) +\n    scale_x_continuous(breaks = seq(2009, 2019, by = 1))+\n   theme(\n    panel.grid.minor = element_blank(),\n    legend.title = element_blank(),\n    axis.title.x = element_blank()\n  )"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#most-expensive-books",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#most-expensive-books",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Most Expensive Books",
    "text": "Most Expensive Books\nbestsellers %>% \n  select(Name, Price) %>% \n  arrange(desc(Price)) %>% \n  head(20) %>% \n  distinct() %>% \n    ggplot(aes(x =reorder(Name, Price), y = Price,\n              fill = ifelse(Price == max(Price), \"red\",\"grey\"))) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#000000\", \"#ff9900\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"none\",\n    axis.title.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#most-instances-on-the-best-sellers-list",
    "href": "posts/amazon-bestsellers-data-analysis/amazon-bestsellers-data-analysis.html#most-instances-on-the-best-sellers-list",
    "title": "Amazon Bestsellers Data Analysis",
    "section": "Most instances on the best sellers list",
    "text": "Most instances on the best sellers list\nbestsellers %>% \n  group_by(Name) %>% \n  summarise(count = n()) %>% \n  arrange(desc(count)) %>% \n  head(10) %>% \n  ggplot(aes(x =reorder(Name, count), y = count,\n              fill = ifelse(count == max(count), \"red\",\"grey\"))) +\n    scale_y_continuous(breaks = seq(0, 13, by = 1))+\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\"#000000\", \"#ff9900\")) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major.y = element_blank(),\n    legend.position = \"none\",\n    axis.title.y = element_blank()\n  )"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html",
    "title": "Web scraping with R and RVest",
    "section": "",
    "text": "Image from Reddit.com by u/booooooop123"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#find-navigate-to-the-url",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#find-navigate-to-the-url",
    "title": "Web scraping with R and RVest",
    "section": "Find & navigate to the URL",
    "text": "Find & navigate to the URL\nTo find the exact url of the page that we want to extract the data from, we first look at the South Park Homepage on IMDB. This page contains the rating and vote count of the South Park TV Series, but it does not contain the ratings and votes of the seasons of the TV Series for each individual episode/season.\nOn the top of the page, we see the list of episodes of South Park, and clicking that link takes me to the latest season of South Park, which is 25 at the time of writing this post.\nFor simplicity, we will start with the first season, which is under the link Season 1."
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#parse-the-url",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#parse-the-url",
    "title": "Web scraping with R and RVest",
    "section": "Parse the URL",
    "text": "Parse the URL\nUsing the read_html function, we can parse the HTML code of the page\n\nlink <- \"https://www.imdb.com/title/tt0121955/episodes?season=1\"\npage <- read_html(link)\n\nThis page should now appear in the R console environment. We can see that the page contains a list of HTML elements. It can take a minute to get the page, depending on your internet connection/processor speed."
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#extract-each-element",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#extract-each-element",
    "title": "Web scraping with R and RVest",
    "section": "Extract each element",
    "text": "Extract each element\nUsing the CSS Selector Gadget, we can find the elements we want to scrape. The following code snippet will find the elements that contain the TV Series name, the TV Series rating, the TV Series votes and the TV Series year and then it will extract the text from each of them.\n\nEpisode Name\n    episode_name <- page |>\n        html_nodes(\"#episodes_content strong a\") |>\n        html_text()\n\n\nEpisode Number\n    episode_season <- page |>\n        html_nodes(\".zero-z-index div\") |>\n        html_text()\n\n\nEpisode Rating\nrating <- page |>\n  html_nodes(\".ipl-rating-star.small .ipl-rating-star__rating\") |>\n        html_text()\n\n\nEpisode Votes\n    total_votes <- page |>\n        html_nodes(\".ipl-rating-star__total-votes\") |>\n        html_text()\n\n\nAir Date\n\n    air_date <- page |>\n        html_nodes(\".airdate\") |>\n        html_text()\n\n\nEpisode Description\n    description <- page |>\n        html_nodes(\".item_description\") |>\n        html_text()\n\n\nRepeat for each season\nThe codes above will scrape the data for one season. To scrape the data for all seasons, we need to repeat the above steps for each season. To do that, we need to use the for loop. Additionally, we need to change the url variable to point to the correct URL for each season."
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#final-code",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#final-code",
    "title": "Web scraping with R and RVest",
    "section": "Final Code",
    "text": "Final Code\nThe final code snippet will look like the following:\n\nlibrary(rvest)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()         masks stats::filter()\n✖ readr::guess_encoding() masks rvest::guess_encoding()\n✖ dplyr::lag()            masks stats::lag()\n\nsouth_park <- data.frame()\n\nfor (seasons in seq(from = 1, to = 25, by = 1)) {\n    link <- paste0(\"https://www.imdb.com/title/tt0121955/episodes?season=\", seasons)\n\n    page <- read_html(link)\n\n    episode_name <- page |>\n        html_nodes(\"#episodes_content strong a\") |>\n        html_text()\n\n    rating <- page |>\n        html_nodes(\".ipl-rating-star.small .ipl-rating-star__rating\") |>\n        html_text()\n\n    total_votes <- page |>\n        html_nodes(\".ipl-rating-star__total-votes\") |>\n        html_text()\n\n    air_date <- page |>\n        html_nodes(\".airdate\") |>\n        html_text()\n\n    description <- page |>\n        html_nodes(\".item_description\") |>\n        html_text()\n\n    episode_season <- page |>\n        html_nodes(\".zero-z-index div\") |>\n        html_text()\n\n    south_park <- rbind(south_park, data.frame(episode_name, episode_season,\n        rating,\n        total_votes,\n        air_date,\n        description,\n        stringsAsFactors = FALSE\n    ))\n}"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#make-the-dataframe",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#make-the-dataframe",
    "title": "Web scraping with R and RVest",
    "section": "Make the Dataframe",
    "text": "Make the Dataframe\nThe final code snippet will create a data frame with the following columns:\n\nepisode_name\nepisode_season\nrating\ntotal_votes\nair_date\ndescription\n\nand save it under the variable south_park.\nLet use look at the data frame.\n\nsouth_park  |> head(10)  |> gt::gt()  |> gtExtras::gt_theme_538()\n\n\n\n\n\n  \n  \n    \n      episode_name\n      episode_season\n      rating\n      total_votes\n      air_date\n      description\n    \n  \n  \n    Unaired Pilot\nS1, Ep0\n7.6\n(816)\n\n            26 Sep. 2019\n    \n\nCartman wakes up and realizes he was abducted by aliens after hearing it from his friends. He is in major denial at first though, but even Chef is a believer.    \n    Cartman Gets an Anal Probe\nS1, Ep1\n7.9\n(4,459)\n\n            13 Aug. 1997\n    \n\nCartman's dream about being abducted by aliens turns out to have actually happened, and when the aliens take Kyle's brother, all of them must find a way to bring the aliens back and confront them.    \n    Weight Gain 4000\nS1, Ep2\n7.6\n(3,518)\n\n            27 Aug. 1997\n    \n\nKathie Lee Gifford comes to South Park to present an award to Cartman, and Mr. Garrison hopes to use the event to assassinate her.    \n    Volcano\nS1, Ep3\n7.7\n(3,398)\n\n            20 Aug. 1997\n    \n\nThe kids go hunting with the rather trigger happy Jimbo and Ned. Little do they know that a volcano is on the verge of erupting.    \n    Big Gay Al's Big Gay Boat Ride\nS1, Ep4\n7.6\n(3,286)\n\n            3 Sep. 1997\n    \n\nStan becomes distracted from the upcoming football game because his dog is gay, so he turns to the gayest man in town for advice, Big Gay Al.    \n    An Elephant Makes Love to a Pig\nS1, Ep5\n7.5\n(3,049)\n\n            10 Sep. 1997\n    \n\nKyle tries to crossbreed an elephant with a pig to win a science contest.    \n    Death\nS1, Ep6\n8.0\n(3,128)\n\n            17 Sep. 1997\n    \n\nGrampa wants Stan to kill him, the parents all want to kill the Terrance and Philip show, and Death just wants to kill someone.    \n    Pinkeye\nS1, Ep7\n8.3\n(3,262)\n\n            29 Oct. 1997\n    \n\nThe living dead beset South Park after a bottle of Worcestershire sauce mixes with embalming fluid at the mortuary; Tina Yothers judged a costume contest.    \n    Starvin' Marvin\nS1, Ep8\n8.1\n(3,141)\n\n            19 Nov. 1997\n    \n\nThe boys sponsor a starving Ethiopian child, only to have him show up on their doorstep.    \n    Mr. Hankey, the Christmas Poo\nS1, Ep9\n8.1\n(3,152)\n\n            17 Dec. 1997\n    \n\nThe town is forced to remove anything that either has anything to do with Christmas or is offensive in the least bit to anyone. And Kyle tries to convince everyone of the existence of \"Mr. Hankey, the Christmas Poo.\"    \n  \n  \n  \n\n\n\n\nThe scraped data is now in the south_park data frame, and can be saved from here. However, to further analyze the data, it needs to be cleaned up."
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#total-votes-to-integer",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#total-votes-to-integer",
    "title": "Web scraping with R and RVest",
    "section": "Total Votes to Integer",
    "text": "Total Votes to Integer\nRemove the () from the total votes string, and convert it into an integer type.\nsouth_park$total_votes <- south_park$total_votes |>\n    str_replace_all(\"\\\\(|\\\\)|\\\\,\", \"\") |>\n    as.integer()"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#rating-to-a-double",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#rating-to-a-double",
    "title": "Web scraping with R and RVest",
    "section": "Rating to a double",
    "text": "Rating to a double\nsouth_park$rating <- as.double(south_park$rating)"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#air_date-to-a-date",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#air_date-to-a-date",
    "title": "Web scraping with R and RVest",
    "section": "Air_date to a Date",
    "text": "Air_date to a Date\nsouth_park$air_date <-\n    str_replace_all(south_park$air_date, \"\\\\n\", \"\") |>\n    trimws() |>\n    lubridate::dmy()"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#description-column-cleaning",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#description-column-cleaning",
    "title": "Web scraping with R and RVest",
    "section": "Description column cleaning",
    "text": "Description column cleaning\nThe description column contains a new line, so need to remove it.\nsouth_park$description <-\n    str_replace_all(south_park$description, \"\\\\n\", \"\")"
  },
  {
    "objectID": "posts/webscraping-with-rvest/rvest-imdb.html#save-the-data",
    "href": "posts/webscraping-with-rvest/rvest-imdb.html#save-the-data",
    "title": "Web scraping with R and RVest",
    "section": "Save the data",
    "text": "Save the data\nFinally save the data as a new dataframe:\nclean_sp <- south_park |>\n    mutate(\n        season = str_extract(episode_season, \"(?<=S)[:digit:]+\"),\n        episode = str_extract(episode_season, \"[:digit:]+$\")\n    ) |>\n    relocate(season, .before = rating) |>\n    relocate(episode, .before = rating) |>\n    select(-episode_season) |>\n    mutate(\n        id = row_number()\n    )"
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome to karat_codes",
    "section": "",
    "text": "Its based on a custom css theme that I am constantly tweaking.\nSource code for everything available on my github.\ngithub.com/SidhuK\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{sidhu2020,\n  author = {Karat Sidhu},\n  editor = {},\n  title = {Welcome to Karat\\_codes},\n  date = {2020-05-21},\n  url = {https://karatsidhu.com/posts/welcome/welcome.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKarat Sidhu. 2020. “Welcome to Karat_codes.” May 21, 2020.\nhttps://karatsidhu.com/posts/welcome/welcome.html."
  },
  {
    "objectID": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html",
    "href": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html",
    "title": "New York Times Bestsellers",
    "section": "",
    "text": "Looking at the New York Times Bestsellers by Decade; data by Post45"
  },
  {
    "objectID": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#loading-libraries",
    "href": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#loading-libraries",
    "title": "New York Times Bestsellers",
    "section": "Loading Libraries",
    "text": "Loading Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nshowtext_opts(dpi = 450)\nshowtext_auto(enable = TRUE)\nlibrary(ggtext)\nlibrary(ggrepel)\nfont_add_google(family = \"Roboto\", name = \"Roboto\")\nfont_add_google(family = \"Outfit\", name = \"Outfit\")"
  },
  {
    "objectID": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#loading-data",
    "href": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#loading-data",
    "title": "New York Times Bestsellers",
    "section": "Loading Data",
    "text": "Loading Data\n\nnyt_titles <-\n    readr::read_tsv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_titles.tsv\")\n\nRows: 7431 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): title, author\ndbl  (5): id, year, total_weeks, debut_rank, best_rank\ndate (1): first_week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nnyt_full <-\n    readr::read_tsv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-10/nyt_full.tsv\")\n\nRows: 60386 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (2): title, author\ndbl  (3): year, rank, title_id\ndate (1): week\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolors_legend <- c(\n    \"#FFB327\",\n    \"#08748f\",\n    \"#4F607C\",\n    \"#2d6554\",\n    \"#8E038E\",\n    \"#5A6D87\",\n    \"#000000\",\n    \"#725050\",\n    \"#542ea5\",\n    \"#304d30\",\n    \"#8E038E\"\n)"
  },
  {
    "objectID": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#data-cleaning-and-basic-eda",
    "href": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#data-cleaning-and-basic-eda",
    "title": "New York Times Bestsellers",
    "section": "Data Cleaning and basic EDA",
    "text": "Data Cleaning and basic EDA\n\nnytitiles_weeks <- nyt_titles %>%\n    mutate(\n        decade = (year %/% 10) * 10\n    ) %>%\n    group_by(decade) %>%\n    slice(which.max(total_weeks)) %>%\n    mutate(title_new = paste(title, \"(\", total_weeks, \"Weeks )\"))\nnytitiles_weeks\n\n# A tibble: 10 × 10\n# Groups:   decade [10]\n      id title   author  year total_weeks first_week debut_rank best_rank decade\n   <dbl> <chr>   <chr>  <dbl>       <dbl> <date>          <dbl>     <dbl>  <dbl>\n 1   481 ANTHON… Herve…  1933          86 1933-07-03          1         1   1930\n 2  6157 THE RO… Lloyd…  1942         111 1942-11-09          5         1   1940\n 3  4749 THE CA… Herma…  1951         123 1951-04-22         13         1   1950\n 4  6887 TO KIL… Harpe…  1960          98 1960-08-07         14         2   1960\n 5  2363 ILLUSI… Richa…  1977          80 1977-06-19         10         2   1970\n 6  4679 THE BO… Tom W…  1987          56 1987-11-08          1         1   1980\n 7  3343 OH, TH… Dr. S…  1990         178 1990-02-25         14         1   1990\n 8  4918 THE DA… Dan B…  2003         165 2003-04-06          9         1   2000\n 9   381 ALL TH… Antho…  2014         132 2014-05-25          2         1   2010\n10   414 AMERIC… Jeani…  2020          34 2020-02-09         12         1   2020\n# … with 1 more variable: title_new <chr>"
  },
  {
    "objectID": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#making-the-plot",
    "href": "posts/new-york-times-bestsellers/new-york-times-bestsellers.html#making-the-plot",
    "title": "New York Times Bestsellers",
    "section": "Making the plot",
    "text": "Making the plot\n\nnyt_titles %>%\n    mutate(\n        decade = (year %/% 10) * 10\n    ) %>%\n    ggplot(aes(\n        x = as.factor(reorder(decade, -decade)),\n        y = total_weeks,\n        group = decade,\n        color = as.factor(decade)\n    )) +\n    geom_jitter(width = 0.25, alpha = 0.2, size = 0.85) +\n    geom_text_repel(aes(\n        label = title_new,\n        family = \"Outfit\"\n    ),\n    size = 4.5,\n    data = nytitiles_weeks, alpha = 0.9,\n    box.padding = 0.35\n    ) +\n    geom_point(\n        data = nytitiles_weeks,\n        aes(\n            x = as.factor(decade),\n            y = total_weeks\n        ),\n        size = 2\n    ) +\n    theme_minimal() +\n    labs(\n        title = \"NYTIMES BESTSELLERS\",\n        subtitle = \"**Bestsellers; by Decade.** <br>Looking at the number of weeks each book stayed on the NYTimes bestsellers<br> list every decade. Most popular bestselling book with the most weeks on the <br> list is labelled for each decade.\",\n        caption = \"Data: Post45 | Graphic: github.com/SidhuK\"\n    ) +\n    ylab(\"Weeks\") +\n    theme(\n        legend.position = \"none\",\n        panel.grid.major.y = element_blank(),\n        panel.grid.minor.y = element_blank(),\n        plot.title = element_markdown(family = \"Cookie\", size = 10),\n        plot.subtitle = element_markdown(family = \"Outfit\", size = 18),\n        plot.caption = element_markdown(family = \"Outfit\", size = 12),\n        axis.title.y = element_blank(),\n        axis.title.x = element_text(size = 18),\n        axis.text = element_text(size = 14),\n    ) +\n    scale_color_manual(\n        values = colors_legend,\n        name = NULL\n    ) +\n    coord_flip()\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n'Cookie' not found, will use 'sans' instead\n\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\nfamily 'Cookie' not found, will use 'sans' instead\n\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead\n\nWarning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\nfont family 'Cookie' not found, will use 'sans' instead"
  },
  {
    "objectID": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#explore-data",
    "href": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#explore-data",
    "title": "TidyModels - KNN & GLM",
    "section": "Explore data",
    "text": "Explore data\nExploratory data analysis (EDA) is an important part of the modeling process3.I am using the default tidymodels template so will be using the inbuilt EDA performace cells to check the data.\n\npenguins %>%\n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species) +\n  hrbrthemes::theme_ipsum()\n\n\n\n\nWe see a clear relation between the sex of the penguins with and their bill depth. Further, we can also see a relation between the bill length for each of the species looks difference wrt to the other."
  },
  {
    "objectID": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#build-models",
    "href": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#build-models",
    "title": "TidyModels - KNN & GLM",
    "section": "Build models",
    "text": "Build models\nLet’s consider how to spend our data budget4:\n\ncreate training and testing sets\ncreate resampling folds from the training set\n\n\nset.seed(123)\npenguin_split <- initial_split(penguins, strata = sex)\npenguin_train <- training(penguin_split)\npenguin_test <- testing(penguin_split)\n\nset.seed(234)\npenguin_folds <- vfold_cv(penguin_train, strata = sex)\npenguin_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [223/26]> Fold01\n 2 <split [223/26]> Fold02\n 3 <split [223/26]> Fold03\n 4 <split [224/25]> Fold04\n 5 <split [224/25]> Fold05\n 6 <split [224/25]> Fold06\n 7 <split [225/24]> Fold07\n 8 <split [225/24]> Fold08\n 9 <split [225/24]> Fold09\n10 <split [225/24]> Fold10\n\n\nLet’s create a model specification5 for each model we want to try:"
  },
  {
    "objectID": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#glm",
    "href": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#glm",
    "title": "TidyModels - KNN & GLM",
    "section": "GLM",
    "text": "GLM\n\nglm_spec <-\n  logistic_reg() %>%\n  set_engine(\"glm\")"
  },
  {
    "objectID": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#knn",
    "href": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#knn",
    "title": "TidyModels - KNN & GLM",
    "section": "KNN",
    "text": "KNN\n\nknn_spec <- nearest_neighbor() %>%\n  set_engine(\"kknn\") %>%\n  set_mode(\"classification\")\n\nTo set up your modeling code, consider using the parsnip addin6 or the usemodels7 package.\nNow let’s build a model workflow8 combining each model specification with a data preprocessor:\n\npenguin_formula <- sex ~ .\n\nglm_wf    <- workflow(penguin_formula, glm_spec)\n\nknn_wf <- workflow(penguin_formula, knn_spec)\n\nIf your feature engineering needs are more complex than provided by a formula like sex ~ ., use a recipe9"
  },
  {
    "objectID": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#evaluate-models",
    "href": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#evaluate-models",
    "title": "TidyModels - KNN & GLM",
    "section": "Evaluate models",
    "text": "Evaluate models\nThese models have no tuning parameters so we can evaluate them as they are. [Learn about tuning hyperparameters here10\n\ncontrl_preds <- control_resamples(save_pred = TRUE)\n\nglm_rs <- fit_resamples(\n  glm_wf,\n  resamples = penguin_folds,\n  control = contrl_preds\n)\n\n! Fold05: preprocessor 1/1, model 1/1: glm.fit: fitted probabilities numerically 0...\n\nknn_rs <- fit_resamples(\n  knn_wf,\n  resamples = penguin_folds,\n  control = contrl_preds\n)\n\nHow did these two models compare?\n\ncollect_metrics(glm_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.916    10  0.0173 Preprocessor1_Model1\n2 roc_auc  binary     0.975    10  0.0105 Preprocessor1_Model1\n\ncollect_metrics(knn_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy binary     0.912    10 0.0132  Preprocessor1_Model1\n2 roc_auc  binary     0.979    10 0.00697 Preprocessor1_Model1"
  },
  {
    "objectID": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#roc-curve-comparing-the-two-models.",
    "href": "posts/tidymodels-knn-and-glm/tidymodels-knn-and-glm.html#roc-curve-comparing-the-two-models.",
    "title": "TidyModels - KNN & GLM",
    "section": "ROC Curve comparing the two models.",
    "text": "ROC Curve comparing the two models.\n\nbind_rows(\n  collect_predictions(glm_rs) %>%\n    mutate(mod = \"glm\"),\n  collect_predictions(knn_rs) %>%\n    mutate(mod = \"knn\")\n) %>%\n  group_by(mod) %>%\n  roc_curve(sex, .pred_female) %>%\n  autoplot()\n\n\n\n\nThese models perform very similarly, so perhaps we would choose the simpler, linear model. The function last_fit() fits one final time on the training data and evaluates on the testing data. This is the first time we have used the testing data.\n\nfinal_fitted <- last_fit(glm_wf, penguin_split)\ncollect_metrics(final_fitted)  ## metrics evaluated on the *testing* data\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.857 Preprocessor1_Model1\n2 roc_auc  binary         0.937 Preprocessor1_Model1\n\n\nThis object contains a fitted workflow that we can use for prediction.\n\nfinal_wf <- extract_workflow(final_fitted)\npredict(final_wf, penguin_test[59,])\n\n# A tibble: 1 × 1\n  .pred_class\n  <fct>      \n1 male       \n\npenguin_test[59,]\n\n# A tibble: 1 × 7\n  species island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex  \n  <fct>   <fct>           <dbl>         <dbl>            <int>       <int> <fct>\n1 Gentoo  Biscoe           46.5          14.8              217        5200 fema…\n\n\nYou can save this fitted final_wf object to use later with new data, for example with readr::write_rds()."
  },
  {
    "objectID": "posts/tidymodels-xgboost/xgboost.html",
    "href": "posts/tidymodels-xgboost/xgboost.html",
    "title": "TidyModels - XGBoost",
    "section": "",
    "text": "TidyTuesday Volleyball Data\nThis is my ongoing series1 of learning a bit more about how different algorithms in TidyModels packages work and performing some of those with the help of available material online.\nThe source of this code is from a Julie Silgie video of the same dataset2 that I used to follow along.\nI have never had to use XGBoost in my line of work so it was a completely new experience for me.\nI trimmed the data because it takes forever to run the workflow on my local machine, so the model produced is not very good.\nI have linked to the original in the footnotes, please use that tutorial instead. This is very bare-bones.\n\n\nIf anyome decides to use this code, a lot more tuning is necessary before the XGBoost is useful for this dataset.\nNow, onto the coding:"
  },
  {
    "objectID": "posts/tidymodels-xgboost/xgboost.html#taking-only-the-top-1000-rows",
    "href": "posts/tidymodels-xgboost/xgboost.html#taking-only-the-top-1000-rows",
    "title": "TidyModels - XGBoost",
    "section": "Taking only the top 1000 rows",
    "text": "Taking only the top 1000 rows\nthe original dataset is ~80x bigger\n\nvb_matches <- vb_matches |> arrange(desc(w_p1_tot_kills)) |> \n  slice_head(n=1000)"
  },
  {
    "objectID": "posts/tidymodels-xgboost/xgboost.html#cleaning-the-columns-and-combining-some-variables-into-new-total-variables",
    "href": "posts/tidymodels-xgboost/xgboost.html#cleaning-the-columns-and-combining-some-variables-into-new-total-variables",
    "title": "TidyModels - XGBoost",
    "section": "cleaning the columns, and combining some variables into new “total” variables",
    "text": "cleaning the columns, and combining some variables into new “total” variables\n\nvb_parsed <- vb_matches %>%\n  transmute(\n    circuit,\n    gender,\n    year,\n    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,\n    w_kills = w_p1_tot_kills + w_p2_tot_kills,\n    w_errors = w_p1_tot_errors + w_p2_tot_errors,\n    w_aces = w_p1_tot_aces + w_p2_tot_aces,\n    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,\n    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,\n    w_digs = w_p1_tot_digs + w_p2_tot_digs,\n    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,\n    l_kills = l_p1_tot_kills + l_p2_tot_kills,\n    l_errors = l_p1_tot_errors + l_p2_tot_errors,\n    l_aces = l_p1_tot_aces + l_p2_tot_aces,\n    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,\n    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,\n    l_digs = l_p1_tot_digs + l_p2_tot_digs\n  )  |> \n  na.omit()"
  },
  {
    "objectID": "posts/tidymodels-xgboost/xgboost.html#note-takes-a-long-time-to-process.",
    "href": "posts/tidymodels-xgboost/xgboost.html#note-takes-a-long-time-to-process.",
    "title": "TidyModels - XGBoost",
    "section": "Note: Takes a long time to process.",
    "text": "Note: Takes a long time to process.\n\ndoParallel::registerDoParallel()\n\nset.seed(234)\nxgb_res <- tune_grid(\n  xgb_wf,\n  resamples = vb_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nxgb_res\n\n# Tuning results\n# 10-fold cross-validation using stratification \n# A tibble: 10 × 5\n   splits             id     .metrics           .notes           .predictions\n   <list>             <chr>  <list>             <list>           <list>      \n 1 <split [1320/148]> Fold01 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 2 <split [1320/148]> Fold02 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 3 <split [1320/148]> Fold03 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 4 <split [1320/148]> Fold04 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 5 <split [1322/146]> Fold05 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 6 <split [1322/146]> Fold06 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 7 <split [1322/146]> Fold07 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 8 <split [1322/146]> Fold08 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n 9 <split [1322/146]> Fold09 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>    \n10 <split [1322/146]> Fold10 <tibble [60 × 10]> <tibble [0 × 3]> <tibble>"
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "",
    "text": "Making a table using the gt and gtextras package in R to look at the Eurovision winners from 2004 to 2022."
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#loading-libraries",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#loading-libraries",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "Loading Libraries",
    "text": "Loading Libraries\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(gt)\nlibrary(gtExtras)"
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#load-data",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#load-data",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "load data",
    "text": "load data\n\neurovision <-\n  readr::read_csv(\n    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-17/eurovision.csv\"\n  )\n\nRows: 2005 Columns: 18\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): event, host_city, host_country, event_url, section, artist, song, ...\ndbl  (4): year, running_order, total_points, rank\nlgl  (2): qualified, winner\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#cleaning-data",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#cleaning-data",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "Cleaning Data",
    "text": "Cleaning Data\n\neurovision <- eurovision |>\n  select(\n    -c(\n      event_url,\n      artist_url,\n      country_emoji,\n      rank_ordinal,\n      running_order,\n      qualified,\n      event\n    )\n  )"
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#just-the-finale-winners",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#just-the-finale-winners",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "Just the finale winners",
    "text": "Just the finale winners\n\neurovision <- eurovision |>\n  filter(section == \"grand-final\") |>\n  group_by(year) |>\n  mutate(average_points = mean(total_points)) |>\n  ungroup()"
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#data-to-make-the-table",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#data-to-make-the-table",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "Data to make the table",
    "text": "Data to make the table\n\neurovision_table <- eurovision |>\n  group_by(artist_country) |>\n  filter(section == \"grand-final\") |>\n  filter(winner == \"TRUE\") |>\n  select(-c(section, rank, winner)) |>\n  select(year, everything())"
  },
  {
    "objectID": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#making-the-gt-table",
    "href": "posts/looking-at-eurovision-winners-with-gt/looking-at-eurovision-winners-with-gt.html#making-the-gt-table",
    "title": "Eurovision winners with gt() and gtextras()",
    "section": "Making the gt() table",
    "text": "Making the gt() table\n\ntable  <- eurovision_table |>\n  gt() |>\n  gt_color_rows(total_points:average_points,\n                type = \"discrete\",\n                palette = \"ggsci::orange_material\",\n  ) |>\n  gt_img_rows(columns = image_url,\n              img_source = \"web\",\n              height = 30) |>\n  gt_theme_538() |>\n  tab_header(\n    title = \"Eurovision Song Contest Winners 2004 - 2022\",\n    subtitle = \"Winners of the Eurovision Song contest grand finales from 2004 to 2022. An average of 26 countries participated\n    in the song contest and the following countries won after advancing to the final rounds. The table details the winning artists\n    and the total points for the corresponding winners; in addition total scores for each winner and the average score for that year.\n    Data grouped by country.\"\n  ) |>\n  tab_source_note(source_note = md(\n    glue::glue(\n      \"Data : {fontawesome::fa('twitter')} Tanya Shapiro | Graphic : {fontawesome::fa('github')} github.com/SidhuK\"\n    )\n  )) |>\n  tab_footnote(footnote = \"Highest Points by a winner\",\n               locations = cells_row_groups(groups = \"Portugal\")) |>\n  tab_style(locations = cells_title(groups = 'title'), # format the main title\n            style = list(\n              cell_text(\n                font = google_font(name = 'Bebas Neue'),\n                size = px(60),\n                color = 'indianred',\n                weight = 700\n              )\n            )) |>\n  gt_merge_stack(col1 = host_city,\n                 col2 = host_country) |> # trim the table a bit\n  gt_merge_stack(col1 = artist,\n                 col2 = song) |>\n  tab_options(table.background.color = \"#f1ebda\",\n              column_labels.background.color = \"#f1ebda\") # set the bg color\n\nWarning: Domain not specified, defaulting to observed range within each\nspecified column.\n\ntable\n\n\n\n\n\n  \n    \n      Eurovision Song Contest Winners 2004 - 2022\n    \n    \n      Winners of the Eurovision Song contest grand finales from 2004 to 2022. An average of 26 countries participated\n    in the song contest and the following countries won after advancing to the final rounds. The table details the winning artists\n    and the total points for the corresponding winners; in addition total scores for each winner and the average score for that year.\n    Data grouped by country.\n    \n  \n  \n    \n      year\n      host_city\n      artist\n      image_url\n      total_points\n      average_points\n    \n  \n  \n    \n      Ukraine\n    \n    2022\nTurin\nItaly\nKalush Orchestra\nStefania\n\n631\n185.60000\n    2016\nStockholm\nNetherlands\nJamala\nZitti E Buoni\n\n534\n187.38462\n    2004\nIstanbul\nIsrael\nRuslana\nArcade\n\n280\n87.00000\n    \n      Italy\n    \n    2021\nRotterdam\nPortugal\nMåneskin\nTOY\n\n524\n174.00000\n    \n      Netherlands\n    \n    2019\nTel Aviv\nUkraine\nDuncan Laurence\nAmar Pelos Dois\n\n498\n182.92308\n    \n      Israel\n    \n    2018\nLisbon\nSweden\nNetta\n1944\n\n529\n191.84615\n    \n      Portugal1\n    \n    2017\nKyiv\nAustria\nSalvador Sobral\nHeroes\n\n758\n187.38462\n    \n      Sweden\n    \n    2015\nVienna\nDenmark\nMåns Zelmerlöw\nRise Like a Phoenix\n\n365\n85.92593\n    2012\nBaku\nSweden\nLoreen\nOnly Teardrops\n\n372\n93.69231\n    \n      Austria\n    \n    2014\nCopenhagen\nAzerbaijan\nConchita Wurst\nEuphoria\n\n290\n82.53846\n    \n      Denmark\n    \n    2013\nMalmö\nGermany\nEmmelie de Forest\nRunning Scared\n\n281\n87.00000\n    \n      Azerbaijan\n    \n    2011\nDüsseldorf\nNorway\nEll/Nikki\nSatellite\n\n221\n99.76000\n    \n      Germany\n    \n    2010\nOslo\nRussia\nLena\nFairytale\n\n246\n90.48000\n    \n      Norway\n    \n    2009\nMoscow\nSerbia\nAlexander Rybak\nBelieve\n\n387\n97.44000\n    \n      Russia\n    \n    2008\nBelgrade\nFinland\nDima Bilan\nMolitva\n\n272\n99.76000\n    \n      Serbia\n    \n    2007\nHelsinki\nGreece\nMarija Šerifović\nHard Rock Hallelujah\n\n268\n101.50000\n    \n      Finland\n    \n    2006\nAthens\nUkraine\nLordi\nMy Number One\n\n292\n91.83333\n    \n      Greece\n    \n    2005\nKyiv\nTurkey\nHelena Paparizou\nWild Dances\n\n230\n94.25000\n  \n  \n    \n      Data :  Tanya Shapiro | Graphic :  github.com/SidhuK\n    \n  \n  \n    \n      1 Highest Points by a winner"
  },
  {
    "objectID": "posts/make-quarto-website/make-quarto-website.html#source-code",
    "href": "posts/make-quarto-website/make-quarto-website.html#source-code",
    "title": "Converting a distill website to Quarto",
    "section": "Source Code",
    "text": "Source Code\nAnd the source code can be found on my GitHub:\nhttps://github.com/SidhuK/karat_codes"
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html",
    "title": "TidyModels - PCA and UMAP",
    "section": "",
    "text": "Note: This is part 2 of the tidymodels practise. A few of these RMarkdown files were generated on a date different to the publishing date.\nPrincipal Component analysis (and UMAP to a lesser extent) have been extensively used in Biological Sciences and specifically in Omics. Compared to other components of the TidyVerse, I am fairly comfortable using and running the PCA test on a set of data. However, I typically use either the base R (prcom) or PCA Tools1 because generating a series of plots using those packages is fairly straightforward2."
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#most-common-ingredients",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#most-common-ingredients",
    "title": "TidyModels - PCA and UMAP",
    "section": "Most Common Ingredients",
    "text": "Most Common Ingredients\n\nboston_cocktails |>\n  count(ingredient, sort = TRUE)\n\n# A tibble: 569 × 2\n   ingredient            n\n   <chr>             <int>\n 1 Gin                 176\n 2 Fresh lemon juice   138\n 3 Simple Syrup        115\n 4 Vodka               114\n 5 Light Rum           113\n 6 Dry Vermouth        107\n 7 Fresh Lime Juice    107\n 8 Triple Sec          107\n 9 Powdered Sugar       90\n10 Grenadine            85\n# … with 559 more rows\n\n\nA lot of cocktails look like are using Gin, Lemon Juice, Syrup and Vodka as some of the most common ingredients."
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#column-cleaning-and-data-conversion",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#column-cleaning-and-data-conversion",
    "title": "TidyModels - PCA and UMAP",
    "section": "Column Cleaning and data conversion",
    "text": "Column Cleaning and data conversion\nThe data isn’t very clean yet. Some ingredients, which are clearly the same are labelled slightly differently. So we need to clean and make the data usable for our analysis.\nSome of the main changes needed in the ingredients:\n\nturn all ingredients to lowercase\n“-” needs to be chagned to ” ”\n“lemon” to “lemon Juice”\n“lime” to “lime juice”\n“grapefruit” to “grapefruit juice” (same for orange)\n\nSome of the main changes needed in the measure:\n\n1/2 to 0.5\n3/4 to 0.75\n1/4 to 0.25\nremove words like “dash”, and “oz” etc.\nConvert the entire column into a number\n\n\ncocktails <- boston_cocktails |> \n  mutate(\n    ingredient = str_to_lower(ingredient),\n    ingredient = str_replace_all(ingredient, \"-\", \" \"),\n    ingredient = str_remove(ingredient, \" liqueur$\"),\n    ingredient = str_remove(ingredient, \" (if desired)$\"),\n    ingredient = case_when(\n      str_detect(ingredient, \"bitters\") ~ \"bitters\",\n      str_detect(ingredient, \"lemon\") ~ \"lemon juice\",\n      str_detect(ingredient, \"lime\") ~ \"lime juice\",\n      str_detect(ingredient, \"grapefruit\") ~ \"grapefruit juice\",\n      str_detect(ingredient, \"orange\") ~ \"orange juice\",\n      TRUE ~ ingredient\n    ),\n    measure = case_when(\n      str_detect(ingredient, \"bitters\") ~ str_replace(measure, \"oz$\", \"dash\"),\n      TRUE ~ measure\n    ),\n    measure = str_replace(measure, \" ?1/2\", \".5\"),\n    measure = str_replace(measure, \" ?3/4\", \".75\"),\n    measure = str_replace(measure, \" ?1/4\", \".25\"),\n    measure_number = parse_number(measure),\n    measure_number = if_else(str_detect(measure, \"dash$\"), # a few drops = so 0.02 oz\n      measure_number / 50,\n      measure_number\n    )\n  ) |> \n  add_count(ingredient) |> \n  filter(n > 15) |> \n  select(-n) |> \n  distinct(row_id, ingredient, .keep_all = TRUE) |> \n  na.omit()\n\nWarning: 20 parsing failures.\nrow col expected    actual\n637  -- a number For glass\n671  -- a number For glass\n794  -- a number For glass\n878  -- a number For glass\n928  -- a number For glass\n... ... ........ .........\nSee problems(...) for more details.\n\ncocktails\n\n# A tibble: 2,542 × 7\n   name       category row_id ingredient_numb… ingredient measure measure_number\n   <chr>      <chr>     <dbl>            <dbl> <chr>      <chr>            <dbl>\n 1 Gauguin    Cocktai…      1                1 light rum  2 oz              2   \n 2 Gauguin    Cocktai…      1                3 lemon jui… 1 oz              1   \n 3 Gauguin    Cocktai…      1                4 lime juice 1 oz              1   \n 4 Fort Laud… Cocktai…      2                1 light rum  1.5 oz            1.5 \n 5 Fort Laud… Cocktai…      2                2 sweet ver… .5 oz             0.5 \n 6 Fort Laud… Cocktai…      2                3 orange ju… .25 oz            0.25\n 7 Fort Laud… Cocktai…      2                4 lime juice .25 oz            0.25\n 8 Cuban Coc… Cocktai…      4                1 lime juice .5 oz             0.5 \n 9 Cuban Coc… Cocktai…      4                2 powdered … .5 oz             0.5 \n10 Cuban Coc… Cocktai…      4                3 light rum  2 oz              2   \n# … with 2,532 more rows"
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#new-dataframe-with-a-wide-format",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#new-dataframe-with-a-wide-format",
    "title": "TidyModels - PCA and UMAP",
    "section": "New Dataframe with a “wide” format",
    "text": "New Dataframe with a “wide” format\nWe’re now close to beginning our analysis. However, the data is in long format and PCA prefers to get the data wider. So we’ll pivot_wide to make the analysis easier.\n\ncocktails_df <- cocktails |> \n  select(-ingredient_number, -row_id, -measure) |> \n  pivot_wider(names_from = ingredient, values_from = measure_number, values_fill = 0) |> \n  janitor::clean_names() |> \n  na.omit()\n\ncocktails_df\n\n# A tibble: 937 × 42\n   name    category light_rum lemon_juice lime_juice sweet_vermouth orange_juice\n   <chr>   <chr>        <dbl>       <dbl>      <dbl>          <dbl>        <dbl>\n 1 Gauguin Cocktai…      2           1          1               0           0   \n 2 Fort L… Cocktai…      1.5         0          0.25            0.5         0.25\n 3 Cuban … Cocktai…      2           0          0.5             0           0   \n 4 Cool C… Cocktai…      0           0          0               0           1   \n 5 John C… Whiskies      0           1          0               0           0   \n 6 Cherry… Cocktai…      1.25        0          0               0           0   \n 7 Casa B… Cocktai…      2           0          1.5             0           0   \n 8 Caribb… Cocktai…      0.5         0          0               0           0   \n 9 Amber … Cordial…      0           0.25       0               0           0   \n10 The Jo… Whiskies      0           0.5        0               0           0   \n# … with 927 more rows, and 35 more variables: powdered_sugar <dbl>,\n#   dark_rum <dbl>, cranberry_juice <dbl>, pineapple_juice <dbl>,\n#   bourbon_whiskey <dbl>, simple_syrup <dbl>, cherry_flavored_brandy <dbl>,\n#   light_cream <dbl>, triple_sec <dbl>, maraschino <dbl>, amaretto <dbl>,\n#   grenadine <dbl>, apple_brandy <dbl>, brandy <dbl>, gin <dbl>,\n#   anisette <dbl>, dry_vermouth <dbl>, apricot_flavored_brandy <dbl>,\n#   bitters <dbl>, straight_rye_whiskey <dbl>, benedictine <dbl>, …\n\n\nThis will be the starting dataset used to run PCA (and UMAP)"
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#prepare-the-analysis",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#prepare-the-analysis",
    "title": "TidyModels - PCA and UMAP",
    "section": "Prepare the analysis",
    "text": "Prepare the analysis\n\nRecipe code\n\npca_rec <- recipe(~., data = cocktails_df) |>  # what data to use\n  update_role(name, category, new_role = \"id\") |> # name and category are identifiers not variables\n  step_normalize(all_predictors()) |> # normalize all other columns\n  step_pca(all_predictors()) # pca for all other columns\n\n\n\nPrep Code\n\npca_prep <- prep(pca_rec)\n\npca_prep\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          2\n predictor         40\n\nTraining data contained 937 data points and no missing data.\n\nOperations:\n\nCentering and scaling for light_rum, lemon_juice, lime_juice, sweet_vermo... [trained]\nPCA extraction with light_rum, lemon_juice, lime_juice, sweet_vermou... [trained]\n\n\nThe prep object now doesn’t contain any output, and we need to tidy the object to read it. List object contains the PCA “results” and components on the 2nd place under “term_info” (open it in the console to see details)."
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#pca-table",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#pca-table",
    "title": "TidyModels - PCA and UMAP",
    "section": "PCA Table",
    "text": "PCA Table\n\ntidied_pca <- tidy(pca_prep, 2)\n\ntidied_pca\n\n# A tibble: 1,600 × 4\n   terms             value component id       \n   <chr>             <dbl> <chr>     <chr>    \n 1 light_rum        0.163  PC1       pca_yx0os\n 2 lemon_juice     -0.0140 PC1       pca_yx0os\n 3 lime_juice       0.224  PC1       pca_yx0os\n 4 sweet_vermouth  -0.0661 PC1       pca_yx0os\n 5 orange_juice     0.0308 PC1       pca_yx0os\n 6 powdered_sugar  -0.476  PC1       pca_yx0os\n 7 dark_rum         0.124  PC1       pca_yx0os\n 8 cranberry_juice  0.0954 PC1       pca_yx0os\n 9 pineapple_juice  0.119  PC1       pca_yx0os\n10 bourbon_whiskey  0.0963 PC1       pca_yx0os\n# … with 1,590 more rows"
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#pca-plots",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#pca-plots",
    "title": "TidyModels - PCA and UMAP",
    "section": "PCA Plots",
    "text": "PCA Plots\nA lot of Principal components seen, plotting the first 4\n\ntidied_pca |> \n  filter(\n    component == \"PC1\" |\n      component == \"PC2\" |\n      component == \"PC3\" |\n      component == \"PC4\"\n   ) |> \n  mutate(component = fct_inorder(component)) |> \n    ggplot(aes(value, terms, fill = terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  hrbrthemes::theme_ipsum() +\n  labs(y = NULL) \n\n\n\n\nLooking at the PC1, simple syrup and powdered sugar are very different from each other, therefore the cocktails must be using one or the other. They also appear to be the two biggest factors.\n\nPC1 and PC2 main contributors\n\ntidied_pca |> \n  filter(component %in% paste0(\"PC\", 1:2)) |> \n  group_by(component) |>\n  top_n(8, abs(value)) |>\n  ungroup() |>\n  mutate(terms = reorder_within(terms, abs(value), component)) |>\n  ggplot(aes(abs(value), terms, fill = value > 0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(\n    x = \"Absolute value of contribution\",\n    y = NULL, fill = \"Positive?\"\n  ) +\n  hrbrthemes::theme_ipsum()\n\n\n\n\n\n\nPCA Plot\n\njuice(pca_prep) |> \n  ggplot(aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  ggrepel::geom_text_repel(max.overlaps = 10) +\n  labs(color = NULL) + hrbrthemes::theme_ipsum()\n\nWarning: ggrepel: 927 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nSimilar kind of cocktails are clustered together, so PCA analysis appears to be somewhat successful."
  },
  {
    "objectID": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#recipe-prep",
    "href": "posts/tidymodels-pca-and-umap/tidymodels-pca-and-umap.html#recipe-prep",
    "title": "TidyModels - PCA and UMAP",
    "section": "Recipe Prep",
    "text": "Recipe Prep\nThe embed package provides recipe steps for ways to create embeddings including UMAP.\n\nlibrary(embed)\n\numap_rec <- recipe(~., data = cocktails_df) |>\n  update_role(name, category, new_role = \"id\") |>\n  step_normalize(all_predictors()) |>\n  step_umap(all_predictors())\n\numap_prep <- prep(umap_rec)\n\numap_prep\n\nRecipe\n\nInputs:\n\n      role #variables\n        id          2\n predictor         40\n\nTraining data contained 937 data points and no missing data.\n\nOperations:\n\nCentering and scaling for light_rum, lemon_juice, lime_juice, sweet_vermo... [trained]\nUMAP embedding for light_rum, lemon_juice, lime_juice, sweet_verm... [trained]\n\n\nSo far, the process is exactly the same as PCA. Prep the recipe and get the “non output”-output.\n\njuice(umap_prep) |> \n  ggplot(aes(UMAP1, UMAP2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 2) +\n  ggrepel::geom_text_repel(max.overlaps = 30) +\n  labs(color = NULL) + hrbrthemes::theme_ipsum()\n\nWarning: ggrepel: 932 unlabeled data points (too many overlaps). Consider\nincreasing max.overlaps\n\n\n\n\n\nSimilarly clustering seen here, but type of cocktails seen here are different."
  },
  {
    "objectID": "posts/mdatools-pls-da-analysis/mdatools-pls-da-analysis.html",
    "href": "posts/mdatools-pls-da-analysis/mdatools-pls-da-analysis.html",
    "title": "MDATools - PLS-DA Analysis",
    "section": "",
    "text": "Footnotes\n\n\nhttps://mdatools.com/docs/plsda.html↩︎\nhttps://github.com/SidhuK/R_for_Metabolomics/tree/main/PLS-DA↩︎\nhttps://towardsdatascience.com/partial-least-squares-f4e6714452a↩︎\nhttps://mdatools.com/docs/plsda–performance-plots.html↩︎\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{sidhu2022,\n  author = {Karat Sidhu},\n  editor = {},\n  title = {MDATools - {PLS-DA} {Analysis}},\n  date = {2022-05-26},\n  url = {https://karatsidhu.com/posts/mdatools-pls-da-analysis/mdatools-pls-da-analysis.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKarat Sidhu. 2022. “MDATools - PLS-DA Analysis.” May 26,\n2022. https://karatsidhu.com/posts/mdatools-pls-da-analysis/mdatools-pls-da-analysis.html."
  },
  {
    "objectID": "posts/netflix-naive_bayes/Naive_bayes.html",
    "href": "posts/netflix-naive_bayes/Naive_bayes.html",
    "title": "Naive Bayes with Tidymodels",
    "section": "",
    "text": "This is part 4 of my Tidymodels learning series. Part 1 dealt with SVM& Random Forests1, part 2 I tried out PCA analysis and UMAP2 and part 3 I wanted to try KNN and GLM modelling3.\nFor the next part, I wanted to try something I usually don’t have to run in metabolomics data, so I went with Naive Bayes.\nData set was used from kaggle.com4.\nNaive Bayes is a basic algorithm for classifying data. It is not often used in my field but is a useful skill to have."
  },
  {
    "objectID": "posts/netflix-naive_bayes/Naive_bayes.html#for-stop-words",
    "href": "posts/netflix-naive_bayes/Naive_bayes.html#for-stop-words",
    "title": "Naive Bayes with Tidymodels",
    "section": "For Stop Words",
    "text": "For Stop Words\n\n\nCode\nrec_stop <-\n  recipe(type ~ description, data = netflix_train) |>\n  step_tokenize(description) |>\n  step_stopwords(description) |>\n  step_tokenfilter(description, max_tokens = 80) |>\n  step_tfidf(description)\n\nrec_stop_norm <-\n  rec_stop |>\n  step_normalize(all_predictors())\n\nrec_stop_smote <-\n  rec_stop_norm |>\n  step_smote(type)\n\n## again, let's check it\nprep(rec_stop_smote)\n\n\nRecipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor          1\n\nTraining data contained 479 data points and no missing data.\n\nOperations:\n\nTokenization for description [trained]\nStop word removal for description [trained]\nText filtering for description [trained]\nTerm frequency-inverse document frequency with description [trained]\nCentering and scaling for tfidf_description_back, tfidf_description_based... [trained]\nSMOTE based on type [trained]"
  },
  {
    "objectID": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html",
    "href": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html",
    "title": "Making streamgraphs in R",
    "section": "",
    "text": "TidyTuesday Dataset for Week of May 03, 2022. The dataset featured the hidden gems files from Berkeley Lab/Dept of Energy."
  },
  {
    "objectID": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html#loading-libraries",
    "href": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html#loading-libraries",
    "title": "Making streamgraphs in R",
    "section": "Loading Libraries",
    "text": "Loading Libraries\n\nlibrary(ggstream)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n\n\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.7     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(patchwork)\nlibrary(showtext)\n\nLoading required package: sysfonts\n\n\nLoading required package: showtextdb\n\nshowtext_opts(dpi = 450)\nshowtext_auto(enable = TRUE)\nlibrary(ggtext)\nfont_add_google(family = \"Bangers\", name = \"Bangers\")"
  },
  {
    "objectID": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html#loading-and-cleaning-data",
    "href": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html#loading-and-cleaning-data",
    "title": "Making streamgraphs in R",
    "section": "Loading and cleaning Data",
    "text": "Loading and cleaning Data\n\ncapacity <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/capacity.csv'\n  )\n\nRows: 49 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): type\ndbl (6): year, standalone_prior, hybrid_prior, standalone_new, hybrid_new, t...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nwind <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/wind.csv'\n  )\n\nRows: 328 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (2): wind_mwh, wind_capacity\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsolar <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/solar.csv'\n  )\n\nRows: 328 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (2): solar_mwh, solar_capacity\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\naverage_cost <-\n  readr::read_csv(\n    'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-05-03/average_cost.csv'\n  )\n\nRows: 13 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): year, gas_mwh, solar_mwh, wind_mwh\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncapacity <- capacity %>%\n  mutate(across(where(anyNA), ~ replace_na(., 0)))\ncapacity <- capacity %>%\n  mutate(prior = standalone_prior + hybrid_prior) %>%\n  mutate(new = standalone_new + hybrid_new)"
  },
  {
    "objectID": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html#making-the-plot",
    "href": "posts/making-streamgraphs-in-r/making-streamgraphs-in-r.html#making-the-plot",
    "title": "Making streamgraphs in R",
    "section": "Making the plot",
    "text": "Making the plot\n\ncolors_legend <- c(\"#FFB327\",\n                   \"#D1F1F9\",\n                   \"#4F607C\",\n                   \"#c5b689\",\n                   \"#8E038E\",\n                   \"#5A6D87\",\n                   \"#000000\")\nplot1 <- ggplot(capacity, aes(year, prior, fill = type)) +\n  geom_stream(\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 1.25,\n    sorting = \"onset\"\n  ) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 0.09,\n    sorting = \"onset\"\n  ) +\n  scale_fill_manual(values = colors_legend,\n                    name = NULL) +\n  scale_x_continuous(breaks = c(2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)) +\n  theme_minimal(base_family = \"Bangers\") +\n  theme(\n    plot.background = element_rect(fill = \"grey84\", color = NA),\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    axis.text.y = element_blank(),\n    legend.position = \"bottom\",\n    legend.text = element_text(color = \"grey40\", size = 14),\n    legend.box.margin = margin(t = 30),\n    legend.background = element_rect(color = \"grey40\",\n                                     size = .3,\n                                     fill = \"grey95\"),\n    legend.key.height = unit(.25, \"lines\"),\n    legend.key.width = unit(2.5, \"lines\"),\n    plot.margin = margin(rep(20, 4))\n  ) +\n  labs(title = \"Prior Generation Capacity (Gigawatts)\") +\n  theme(plot.title = element_text(hjust = 0.5))\nplot1\n\n\n\nplot2 <- ggplot(capacity, aes(year, new, fill = type)) +\n  geom_stream(\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 1.25,\n    sorting = \"onset\"\n  ) +\n  geom_stream(\n    geom = \"contour\",\n    color = \"white\",\n    extra_span = .25,\n    true_range = \"none\",\n    bw = .85,\n    size = 0.02,\n    sorting = \"onset\"\n  ) +\n  scale_fill_manual(values = colors_legend,\n                    name = NULL) +\n  scale_x_continuous(breaks = c(2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020)) +\n  theme_minimal(base_family = \"Bangers\") +\n  theme(\n    plot.background = element_rect(fill = \"grey84\", color = NA),\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    axis.text.y = element_blank(),\n    legend.position = \"none\",\n    plot.margin = margin(rep(20, 4))\n  ) +\n  labs(title = \"New Generation Capacity (Gigawatts)\") +\n  theme(plot.title = element_text(hjust = 0.5))\nplot_final <- plot1 / plot2 +\n  plot_annotation(\n    title = 'Power Generation Capacity',\n    subtitle = 'The shreamcharts describe the power generation from various sources (solar, nuclear, wind, etc) along with <br>\n    their capacity over the years. The graphs are separated into New and Prior Generation <br> In Gigawatts. <br> ',\n    caption = 'Data: Berkeley Lab | Graphic: Github.com/SidhuK ',\n    theme = theme(\n      plot.title = element_text(size = 35, hjust = 0.5),\n      plot.subtitle = element_markdown(size = 15, hjust = 0.5),\n      plot.caption = element_text(size = 11, hjust = 0.5),\n      plot.background = element_rect(fill = \"grey84\", color = NA)\n    )\n  ) &\n  theme(text = element_text('Bangers'))\n\n\nplot_final\n\n\n\n\n\nVisit my TidyTuesday Repo for a better look."
  },
  {
    "objectID": "posts/netflix-original-movies-eda/netflix-original-movies-eda.html",
    "href": "posts/netflix-original-movies-eda/netflix-original-movies-eda.html",
    "title": "Netflix Original Movies - EDA",
    "section": "",
    "text": "Reusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{sidhu2022,\n  author = {Karat Sidhu},\n  editor = {},\n  title = {Netflix {Original} {Movies} - {EDA}},\n  date = {2022-04-21},\n  url = {https://karatsidhu.com/posts/netflix-original-movies-eda/netflix-original-movies-eda.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKarat Sidhu. 2022. “Netflix Original Movies - EDA.” April\n21, 2022. https://karatsidhu.com/posts/netflix-original-movies-eda/netflix-original-movies-eda.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Antony Barja",
    "section": "",
    "text": "Click here to know more about me"
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Karat Sidhu",
    "section": "",
    "text": "Netlify Status"
  },
  {
    "objectID": "readme.html#i-use-the-entire-webpage",
    "href": "readme.html#i-use-the-entire-webpage",
    "title": "Karat Sidhu",
    "section": "I use the entire webpage:",
    "text": "I use the entire webpage:\n\nas an online business card,\nfor people to have a quick & convenient place to get in touch with me\nso people can get familiar with my qualifications and experience and;\nfor journalizing/learning things I am interested in through my blog."
  },
  {
    "objectID": "readme.html#note",
    "href": "readme.html#note",
    "title": "Karat Sidhu",
    "section": "Note:",
    "text": "Note:\nIts a perpetual work in progress because I like to change up the formatting and themes occasionally and add stuff when necessary.\nIf you are interested in using this as a starting point for your own website, you are welcome to do so. Just fork the repo and get editing.\nNo attribution required for the template, but always appreciated.\n\nKarat"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Today I learned",
    "section": "",
    "text": "Web scraping with R and RVest\n\n\nStep by step guide to scraping IMDB for TV Series ratings and votes using R and rvest package\n\n\n\nJun 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Quick Guide to Formatting with Quarto\n\n\nPlaying with quarto settings to learn more about how it works.\n\n\n\nJun 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyModels - XGBoost\n\n\nLearning Tidymodels package using the Volleyball dataset from TidyTuesday.\n\n\n\nJun 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNaive Bayes with Tidymodels\n\n\nUsing Tidymodels to classify Netflix dataset with the help of Naive Bayes classifier.\n\n\n\nMay 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverting a distill website to Quarto\n\n\nUsing Quarto to make a responsive and minimal academic website/blog in RStudio\n\n\n\nMay 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMDATools - PLS-DA Analysis\n\n\nPLS-DA Analysis using the Iris Dataset and MDA-Tools package\n\n\n\nMay 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyModels - KNN & GLM\n\n\nWorking with TidyModels to generate KNN and GLM models for the palmer penguins dataset.\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEPL 2022 Final Table\n\n\nUsing gt package to create a great looking table for the EPL Standings 2021-22.\n\n\n\nMay 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyModels - PCA and UMAP\n\n\nWorking with TidyModels to generate PCA and UMAP for Cocktail recipes. Using these recipes extrapolating the relation of a cocktail to each other.\n\n\n\nMay 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyModels - SVM & Random Forests\n\n\nLearning Tidymodels package using the Chocolates dataset from TidyTuesday.\n\n\n\nMay 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVSCode vs RStudio — Worth the Switch?\n\n\nDoes Microsoft’s flagship code editor hold up against the old favorite?\n\n\n\nMay 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEurovision winners with gt() and gtextras()\n\n\nUsing gt() package to make a great looking table.\n\n\n\nMay 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew York Times Bestsellers\n\n\nWorking with geom_jitter and ggrepel to create a great looking plot.\n\n\n\nMay 10, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nMaking streamgraphs in R\n\n\nTidyTuesday May 03,2022 ; Power and Energy Capacity visualization.\n\n\n\nMay 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon Bestsellers Data Analysis\n\n\nKMeans and Exploratory Data Analysis.\n\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetflix Original Movies - EDA\n\n\nExploratory Data Analysis, and data vis with ggplot2.\n\n\n\nApr 21, 2022\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nWelcome to karat_codes\n\n\nWelcome to my new blog, karat_codes.\n\n\n\nMay 21, 2020\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{untitled,\n  author = {},\n  editor = {},\n  title = {Today {I} Learned},\n  url = {https://karatsidhu.com/blog.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\n“Today I Learned.” n.d. https://karatsidhu.com/blog.html."
  }
]